{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ff8712-121e-4c55-9b57-eb317f36a195",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plan\n",
    "\n",
    "- [x] FL base framework\n",
    "- [x] implement non-iid sampling function following dirichlet dist\n",
    "\n",
    "#### Attack simulation\n",
    "- [x] untargeted model poisoning\n",
    "    - Local model poisoning attacks to Byzantine-robust federated learning <br>\n",
    "    https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning fang attack <br>\n",
    "    krum-attack, trimmed-mean/median attack\n",
    "- [ ] targeted model poisoning\n",
    "    - Analyzing federated learning through an adversarial lens <br>\n",
    "    https://github.com/inspire-group/ModelPoisoning\n",
    "- [ ] data poisoning\n",
    "    - DBA: Distributed backdoor attacks against federated learning <br>\n",
    "    https://github.com/AI-secure/DBA\n",
    "    \n",
    "#### Defense baseline\n",
    "- [x] FedAvg\n",
    "- [x] Krum\n",
    "- [x] Multi-Krum\n",
    "- [x] Bulyan\n",
    "- [x] Coordinate median\n",
    "- [ ] FoolsGold\n",
    "- [ ] FLARE\n",
    "\n",
    "#### Proposed method\n",
    "- [ ] extract PLRs\n",
    "- [ ] compare the hypersphere uniformity loss \n",
    "- [ ] apply RBF hypersphere CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97834e9-666d-4e4a-a15e-332181ee70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e5affa-9f28-4dad-99af-52f285107603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, mal_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, get_mal_dataset, exp_details, flatten, construct_ordered_dict\n",
    "from aggregate import fedavg, multi_krum, krum, coomed, bulyan, flare, fed_align, fed_cc\n",
    "from attacks import get_malicious_updates_untargeted_mkrum, get_malicious_updates_untargeted_med\n",
    "from cka import kernel_CKA\n",
    "# python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80c63c-85e6-433f-b354-b8656f3a2539",
   "metadata": {},
   "source": [
    "# Untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea6ac2c-32ca-417f-90fb-f5b58b7ad1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 50\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 5 # num of local epoch\n",
    "    local_bs = 100 # batch size\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    aggregation = 'fedcc' # fedavg, krum, mkrum, coomed, bulyan, flare, fedcc\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    num_channels = 1 # num of channels of imgs\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'cifar' # fmnist, cifar, mnist\n",
    "    num_classes = 10 \n",
    "    gpu = 0\n",
    "    optimizer = 'adam'\n",
    "    iid = 0 # 0 for non-iid\n",
    "    alpha = 0.2 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_users = [0, 1] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 1 # number of maliciuos data sample\n",
    "    local_mal_ep = 5\n",
    "    boost = 5 # alpha: 2 for fedavg, 3.5 for krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0462f137-23d4-4d56-8de6-1012b0181ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : cnn\n",
      "    Optimizer : adam\n",
      "    Learning  : 0.001\n",
      "    Aggregation     : fedcc\n",
      "    Global Rounds   : 50\n",
      "\n",
      "    Federated parameters:\n",
      "    Non-IID              : 0.2\n",
      "    Fraction of users    : 1\n",
      "    Local Batch size     : 100\n",
      "    Local Epochs         : 5\n",
      "\n",
      "    Malicious parameters:\n",
      "    Attackers            : [0, 1]\n",
      "    Attack Type          : targeted\n",
      "    Mal Boost            : 5\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CNNCifar(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "malcious dataset true labels: 2, malicious labels: [3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "| Global Training Round : 1 |\n",
      "=========================================\n",
      "user 0, loss 8.785504150390626, acc 90.72948328267478, mal loss 5.671874046325684, mal acc 0.0\n",
      "user 1, loss 6.359981822967529, acc 95.81749049429658, mal loss 3.3141536712646484, mal acc 0.0\n",
      "user 2, loss 1.2553675655395753, acc 27.260812581913502\n",
      "user 3, loss 1.2431131457027635, acc 0.0\n",
      "user 4, loss 1.0618263911317896, acc 1.5337423312883436\n",
      "user 5, loss 0.7110698693785175, acc 66.38655462184873\n",
      "user 6, loss 1.0476284375190734, acc 58.95765472312704\n",
      "user 7, loss 1.1427614144980907, acc 38.44221105527638\n",
      "user 8, loss 0.7727232098579407, acc 73.54497354497354\n",
      "user 9, loss 0.5917896860954809, acc 9.936908517350158\n",
      "[0.7915438667770563, 0.8353731051778266, 0.7573748762238459, 0.7923410218552733, 0.8875145880862669, 0.8904223051605455, 0.8781116352940534, 0.8338231385444032, 0.8046339129550272, 0.8179751574505504]\n",
      "Counter({1: 7, 0: 3})\n",
      "len(selected_parameters) = 7\n",
      "fed_cka Selected idx: [0 1 2 3 7 8 9]\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 2.2971765693081383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\cuda113\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "  0%|                                                                                           | 0/50 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvg Training Stats after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m global rounds:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(train_loss))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 183\u001b[0m test_acc, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGlobal model Benign Test Accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mtest_acc))\n\u001b[0;32m    186\u001b[0m mal_acc, mal_loss, mal_out \u001b[38;5;241m=\u001b[39m mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
      "File \u001b[1;32m~\\Documents\\FL\\src\\update.py:209\u001b[0m, in \u001b[0;36mtest_inference\u001b[1;34m(args, model, test_dataset)\u001b[0m\n\u001b[0;32m    205\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    207\u001b[0m testloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[0;32m    210\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Inference\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "\n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "    print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        acts = []\n",
    "\n",
    "        local_weights, local_losses = [], []\n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "        cka_vals = [] \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_users:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "\n",
    "            local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss, act = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            acts.append(act)\n",
    "            \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                \n",
    "            # offsets.append(w[key].detach().cpu().numpy().reshape(-1).shape[0])\n",
    "            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "            flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_users)\n",
    "        # print(only_weights.shape)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "        elif args.aggregation == 'fedalign':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, selected_idxs = fed_align(malicious_grads, plrs)\n",
    "            print(f'fed_cka_align Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]            \n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, selected_idxs = fed_cc(malicious_grads, glob_plr, plrs)\n",
    "            print(f'fed_cka Selected idx: {selected_idxs}')\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "\n",
    "            mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "            print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "\n",
    "#                 print('%s: %s n_attacker %d fed_model val loss %.4f val acc %.4f' \\\n",
    "#                       %(args.aggregation, args.attack_type, n_attacker, test_loss, 100*test_acc))\n",
    "#                 print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "    mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out[0]))\n",
    "    \n",
    "\n",
    "    # Saving the objects train_loss and train_accuracy:\n",
    "    file_name = '../save/objects/{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\\\n",
    "        format(args.dataset, args.model, args.epochs, args.frac, args.iid, args.local_ep, args.local_bs)\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump([train_loss, train_accuracy], f)\n",
    "\n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b1a9a-ddda-4f9b-b2ca-d49ef8e7fcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301f96de-5e01-4567-b110-a1e150a1189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 4, 4])\n",
      "torch.Size([1, 64, 4, 4])\n",
      "torch.Size([3, 64, 4, 4])\n",
      "torch.Size([1, 64, 4, 4])\n",
      "torch.Size([6, 64, 4, 4])\n",
      "torch.Size([59, 64, 4, 4])\n",
      "torch.Size([58, 64, 4, 4])\n",
      "torch.Size([86, 64, 4, 4])\n",
      "torch.Size([40, 64, 4, 4])\n",
      "torch.Size([69, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "avg_acts = []\n",
    "for i in range(len(acts)):\n",
    "    print((acts[i]).shape)\n",
    "    avg_acts.append(np.mean(acts[i].detach().cpu().numpy(), axis = (1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d81ce0-8ba7-4538-b816-40bc434c016e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(1, 4)\n",
      "(3, 4)\n",
      "(1, 4)\n",
      "(6, 4)\n",
      "(59, 4)\n",
      "(58, 4)\n",
      "(86, 4)\n",
      "(40, 4)\n",
      "(69, 4)\n"
     ]
    }
   ],
   "source": [
    "for act in avg_acts:\n",
    "    print(act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d348f7-d7f3-44b9-94e2-e0ced9795903",
   "metadata": {},
   "outputs": [],
   "source": [
    "act1 = acts[0].detach().cpu().numpy()\n",
    "act2 = acts[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc95ef4-07b5-4382-9d1f-38468f20f8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51.,\n",
       "       52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6073317-6a46-4e4a-a990-7d92cc96dcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "525e3452-1ae8-4cfc-b9b6-f58afa042da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confidence = {'fedavg':[],\n",
    "              'krum'  :[],\n",
    "              'coomed':[],\n",
    "              'bulyan':[],\n",
    "              'flare' :[],\n",
    "              'fedcc' :[]}\n",
    "for i in range(10):\n",
    "    confidence['krum'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf27b786-1cd9-4612-a9f5-8eea22fe9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmeans1d\n",
    "\n",
    "x = [0.9850572609232882, 0.9847250213129664, 0.9848672771600722, 0.9997756678639741, 0.9995519971898091, 0.9997412190655622, 0.9999016268180031, 0.9996079270875822, 0.9996735287208167, 0.9996309990394843]\n",
    "k = 2\n",
    "\n",
    "clusters, centroids = kmeans1d.cluster(x, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf7b0316-ed3d-4dfb-a7d3-c8e89f7ac4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45fe37-0b67-486b-9cb9-4ec77ea90048",
   "metadata": {},
   "outputs": [],
   "source": [
    "scal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bba4deb6-388f-4793-ac90-38e8ce947309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3, 1: 7})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16581c87-da57-412b-8bc7-b06451732c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspect = 1\n",
    "clusters.index(suspect)\n",
    "suspects  = [idx for (idx, item) in enumerate(clusters) if item == suspect]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f66cb9b4-32c6-42ac-9bce-58d0e4280939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97edc7d1-2e6b-49ca-970f-0958a2f9f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "712a4e5e-e57b-4c64-be6a-45ce6b356f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "if args.iid:\n",
    "    filename = 'Confidence_IID.csv'\n",
    "else:\n",
    "    filename = 'Confidence_NIID.csv'\n",
    "\n",
    "with open(filename, mode='w') as csv_file:\n",
    "    csv_data = [args.aggregation, confidence]\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(csv_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47035933-587b-4faa-be93-50a01a1995ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When on a regular grid with x.size = m and y.size = n, if z.ndim == 2, then z must have shape (n, m)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m idx2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, act1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], act1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \n\u001b[0;32m     12\u001b[0m arr \u001b[38;5;241m=\u001b[39m act1[d,:,:,c]\n\u001b[1;32m---> 13\u001b[0m f_interp \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# create a larger arr\u001b[39;00m\n\u001b[0;32m     16\u001b[0m large_idx1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, act1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], act1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\scipy\\interpolate\\_interpolate.py:209\u001b[0m, in \u001b[0;36minterp2d.__init__\u001b[1;34m(self, x, y, z, kind, copy, bounds_error, fill_value)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m z\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(y), \u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[1;32m--> 209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen on a regular grid with x.size = m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand y.size = n, if z.ndim == 2, then z \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust have shape (n, m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    213\u001b[0m     j \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(x)\n",
      "\u001b[1;31mValueError\u001b[0m: When on a regular grid with x.size = m and y.size = n, if z.ndim == 2, then z must have shape (n, m)"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "num_d, h, w, _ = acts1.shape\n",
    "num_c = act1.shape[-1]\n",
    "act1_interp = np.zeros((num_d, h, w, num_c))\n",
    "\n",
    "for d in range(num_d):\n",
    "    for c in range(num_c):\n",
    "        # form interpolation function\n",
    "        idx1 = np.linspace(0, act1.shape[1], act1.shape[1], endpoint=False)\n",
    "        idx2 = np.linspace(0, act1.shape[2], act1.shape[2], endpoint=False)  \n",
    "        arr = act1[d,:,:,c]\n",
    "        f_interp = interpolate.interp2d(idx1, idx2, arr)\n",
    "        \n",
    "        # create a larger arr\n",
    "        large_idx1 = np.linspace(0, act1.shape[1], act1.shape[1], endpoint=False)\n",
    "        large_idx2 = np.linspace(0, act1.shape[2], act1.shape[2], endpoint=False)  \n",
    "        act1interp[d,:,:,c] = f_interp(large_idx1, large_idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72233ba6-a77d-4bde-8dfc-f18726414d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\cuda113\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\user\\anaconda3\\envs\\cuda113\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (6,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mnum_users):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mnum_users):\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mkernel_CKA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg_acts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_acts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\FL\\src\\cka.py:53\u001b[0m, in \u001b[0;36mkernel_CKA\u001b[1;34m(X, Y, sigma)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkernel_CKA\u001b[39m(X, Y, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 53\u001b[0m     hsic \u001b[38;5;241m=\u001b[39m \u001b[43mkernel_HSIC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     var1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(kernel_HSIC(X, X, sigma))\n\u001b[0;32m     55\u001b[0m     var2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(kernel_HSIC(Y, Y, sigma))\n",
      "File \u001b[1;32m~\\Documents\\FL\\src\\cka.py:35\u001b[0m, in \u001b[0;36mkernel_HSIC\u001b[1;34m(X, Y, sigma)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkernel_HSIC\u001b[39m(X, Y, sigma):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mcentering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrbf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcentering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrbf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (6,6) "
     ]
    }
   ],
   "source": [
    "\n",
    "# kernel CKA on pairwise local client's output of the activation \n",
    "for i in range(args.num_users):\n",
    "    for j in range(i + 1, args.num_users):\n",
    "        print(kernel_CKA(avg_acts[i], avg_acts[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f54198-e4f6-4ed7-ab28-5694cc68ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "CNNCifar(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fea4d-0250-4cb7-a74f-57b193e3f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mal_datset_of_class\n",
    "\n",
    "test_mal_X_list, test_mal_Y, test_Y_true = get_mal_dataset_of_class(test_dataset, args.num_mal*100, Y_true, mal_Y)\n",
    "flat_test_mal_Y = np.hstack([y for y in test_mal_Y])\n",
    "mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, test_mal_X_list, flat_test_mal_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdedac-49b9-4fa4-9db1-c029f6246e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_acc, mal_loss, torch.mean(mal_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bbe84-5615-4ae0-8a0c-2eb42988a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PLOTTING (optional)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.figure()\n",
    "plt.title('Training Loss vs Communication rounds')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))\n",
    "\n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61762571-fb22-4871-ac3d-44ae3d5caa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = list(local_weights[0].keys())[-2]\n",
    "second_last_layer = list(local_weights[0].keys())[-4]\n",
    "thrid_last_layer = list(local_weights[0].keys())[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90225f03-4240-4ca5-bfb9-3d3b9a69fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape the flattened global weights into the ordereddict\n",
    "keys = global_model.state_dict().keys()\n",
    "structured_local_weights = []\n",
    "\n",
    "for grad in enumerate(malicious_grads):\n",
    "    start_idx = 0\n",
    "    model_grads = []\n",
    "\n",
    "    for i, param in enumerate(global_model.parameters()):\n",
    "        param_ = grad[1][start_idx:start_idx + len(param.data.view(-1))].reshape(param.data.shape)\n",
    "        start_idx = start_idx + len(param.data.view(-1))\n",
    "        param_ = param_.cuda()\n",
    "        model_grads.append(param_)\n",
    "    structured_local_weights.append(OrderedDict(dict(zip(keys, model_grads))))\n",
    "# structured_local_weights = OrderedDict(dict(zip(keys, model_grads)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d591d-d7f0-4672-b157-3efc842c7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in structured_local_weights[0].keys():\n",
    "    if 'weight' in key:\n",
    "        print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a926d5-246f-401e-9bb2-32c568db9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = list(local_weights[0].keys())[-2]\n",
    "second_last_layer = list(local_weights[0].keys())[-4]\n",
    "thrid_last_layer = list(local_weights[0].keys())[-6]\n",
    "\n",
    "# list of the second last layer's weights\n",
    "plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e260f6e-1c9c-4843-bee4-13bcfb0e001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz : batch size (number of positive pairs)\n",
    "# d   : latent dim\n",
    "# x   : Tensor, shape=[bsz, d]\n",
    "#       latents for one side of positive pairs\n",
    "# y   : Tensor, shape=[bsz, d]\n",
    "#       latents for the other side of positive pairs\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff836bb-f2c6-43d6-876e-dc1f3f8a5d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(plrs)):\n",
    "    print(f'uniform loss plrs[{i}]: {uniform_loss(plrs[i])}')\n",
    "    for j in range(0, len(plrs)):\n",
    "        print(f'align_loss btwn plrs[{i}] and [{j}]: {align_loss(plrs[i], plrs[j])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2ec6d-afeb-408b-aabf-e262d1daed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cka import linear_CKA, kernel_CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be54f31-3a95-4fd6-bc10-ec7964677f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100, 64)\n",
    "Y = np.random.randn(100, 64)\n",
    "\n",
    "print('Linear CKA, between X and Y: {}'.format(linear_CKA(X, Y)))\n",
    "print('Linear CKA, between X and X: {}'.format(linear_CKA(X, X)))\n",
    "\n",
    "print('RBF Kernel CKA, between X and Y: {}'.format(kernel_CKA(X, Y)))\n",
    "print('RBF Kernel CKA, between X and X: {}'.format(kernel_CKA(X, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ef250-4b29-4f3c-8696-5aefd03a7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plrs = [plr.cpu().detach() for plr in plrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eee470-a412-4cf2-bfee-3a8579d1981f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(malicious_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad12ca-9341-4704-9598-a233638517f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_mal_grad = malicious_grads.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4d890-fc91-433a-977b-e43b72a430c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_mal_grad[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fbae2-324e-4e96-9c13-7f9fbbe4bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(detached_mal_grad[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebee882-34eb-4c92-92fd-28c95e9dec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_mal_grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae11e27-f0c2-4a4b-a645-19c09c0bfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fad7dd-c1fb-462e-b789-77cf4645fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_CKA(np.array(detached_mal_grad[1]), np.array(detached_mal_grad[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618b6be-d513-4d45-ad10-9a58ee4c861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(plrs)):\n",
    "    for j in range(i + 1, len(plrs)):\n",
    "        print(f'kernel_cka(all[{i}], all[{j}]: {kernel_CKA(plrs[i], plrs[j])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e95dd-0a30-435e-827b-124b066df3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_CKA(plrs[9].cpu().detach(), plrs[8].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9d209-da55-4f8b-8326-dc811c6ab8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
