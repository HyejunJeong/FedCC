{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ff8712-121e-4c55-9b57-eb317f36a195",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plan\n",
    "\n",
    "- [x] FL base framework\n",
    "- [x] implement non-iid sampling function following dirichlet dist\n",
    "\n",
    "#### Attack simulation\n",
    "- [x] untargeted model poisoning\n",
    "    - Local model poisoning attacks to Byzantine-robust federated learning <br>\n",
    "    https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning fang attack <br>\n",
    "    krum-attack, trimmed-mean/median attack\n",
    "- [x] targeted model poisoning\n",
    "    - Analyzing federated learning through an adversarial lens <br>\n",
    "    https://github.com/inspire-group/ModelPoisoning\n",
    "- [x] data poisoning\n",
    "    - DBA: Distributed backdoor attacks against federated learning <br>\n",
    "    https://github.com/AI-secure/DBA\n",
    "    \n",
    "#### Defense baseline\n",
    "- [x] FedAvg\n",
    "- [x] Krum\n",
    "- [x] Multi-Krum\n",
    "- [x] Bulyan\n",
    "- [x] Coordinate median\n",
    "- [x] FLARE\n",
    "\n",
    "#### Proposed method\n",
    "- [x] extract PLRs\n",
    "- [x] apply RBF hypersphere CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97834e9-666d-4e4a-a15e-332181ee70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e5affa-9f28-4dad-99af-52f285107603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, mal_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, Alexnet, modelC, LeNet5\n",
    "from utils import get_dataset, get_mal_dataset, exp_details, flatten, construct_ordered_dict\n",
    "from aggregate import fedavg, multi_krum, krum, coomed, bulyan, tr_mean, fed_align, fed_cc, flare, fltrust\n",
    "from attacks import get_malicious_updates_untargeted_mkrum, get_malicious_updates_untargeted_med, get_malicious_updates_targeted\n",
    "from cka import linear_CKA, kernel_CKA\n",
    "# python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=10\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5ee40-5071-437b-bc8e-35135865c450",
   "metadata": {},
   "source": [
    "# Targeted Model Poisoning Attack (label flipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea6ac2c-32ca-417f-90fb-f5b58b7ad1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 40\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 3 # num of local epoch\n",
    "    local_bs = 100 # batch size\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    aggregation = 'mkrum' # trmean, flare, fltrust, fedcc fedavg, krum, mkrum, trmean, coomed, bulyan, flare, fedcc, fltrust\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'cifar' # fmnist, cifar, mnist\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes = 100 \n",
    "        num_channels = 3 # num of channels of imgs\n",
    "    else:\n",
    "        num_classes = 10\n",
    "        num_channels = 1\n",
    "    \n",
    "    gpu = 0\n",
    "    optimizer = 'adam'\n",
    "    iid = 1 # 0 for non-iid\n",
    "    alpha = 1 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_clients = [0] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 5 # number of maliciuos data sample\n",
    "    mal_bs = 100\n",
    "    mal_lr = 0.005\n",
    "    mal_test_bs = 100\n",
    "    local_mal_ep = 6\n",
    "    boost = 5 # alpha: 2 for fedavg, 3.5 for krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0462f137-23d4-4d56-8de6-1012b0181ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : cnn\n",
      "    Optimizer : adam\n",
      "    Learning  : 0.001\n",
      "    Aggregation     : mkrum\n",
      "    Global Rounds   : 40\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    Fraction of users    : 1\n",
      "    Local Batch size     : 100\n",
      "    Local Epochs         : 3\n",
      "\n",
      "    Malicious parameters:\n",
      "    Attackers            : [0]\n",
      "    Attack Type          : targeted\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CNNCifar(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "malcious dataset true labels: 8, malicious labels: [5, 5, 5, 5, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "| Global Training Round : 1 |\n",
      "=========================================\n",
      "user 0, loss 2.0746984132906285, acc 14.6, mal loss 0.19353358447551727, mal acc 100.0\n",
      "user 1, loss 1.872601819038391, acc 40.8\n",
      "user 2, loss 1.827405075232188, acc 42.199999999999996\n",
      "user 3, loss 1.8372954636812209, acc 38.800000000000004\n",
      "user 4, loss 1.8339323113361994, acc 42.8\n",
      "user 5, loss 1.8563016643126806, acc 42.0\n",
      "user 6, loss 1.83305160899957, acc 43.2\n",
      "user 7, loss 1.8287091155846913, acc 44.6\n",
      "user 8, loss 1.8803029119968413, acc 40.400000000000006\n",
      "user 9, loss 1.8513404270013174, acc 39.0\n",
      "multiKrum Selected idxs: [1 3 0 9 7 6 4 2 5]\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 1.8695638810473731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:17<11:03, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 27.40% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 2.48, confidence: 0.1802783727645874\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 2 |\n",
      "=========================================\n",
      "user 0, loss 1.8168464734302308, acc 25.8, mal loss 0.26052623987197876, mal acc 100.0\n",
      "user 1, loss 1.5545204003651936, acc 47.599999999999994\n",
      "user 2, loss 1.5385595947504045, acc 47.0\n",
      "user 3, loss 1.5133446007966995, acc 46.6\n",
      "user 4, loss 1.5226534475882847, acc 48.0\n",
      "user 5, loss 1.5724393685658773, acc 44.6\n",
      "user 6, loss 1.5274298171202343, acc 47.4\n",
      "user 7, loss 1.5165164748827618, acc 49.4\n",
      "user 8, loss 1.5459824472665786, acc 43.8\n",
      "user 9, loss 1.543866604566574, acc 44.0\n",
      "multiKrum Selected idxs: [1 9 3 0 4 6 7 2 5]\n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training Loss : 1.7173899019903285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:34<10:51, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 40.32% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 2.24, confidence: 0.18700044155120848\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 3 |\n",
      "=========================================\n",
      "user 0, loss 1.55449451033662, acc 38.0, mal loss 0.232404425740242, mal acc 100.0\n",
      "user 1, loss 1.3471052745978038, acc 52.2\n",
      "user 2, loss 1.3602280457814535, acc 54.400000000000006\n",
      "user 3, loss 1.3475637982288997, acc 50.0\n",
      "user 4, loss 1.347462483247121, acc 48.4\n",
      "user 5, loss 1.3766907622416813, acc 50.8\n",
      "user 6, loss 1.3551732937494914, acc 52.6\n",
      "user 7, loss 1.327865754564603, acc 52.800000000000004\n",
      "user 8, loss 1.3527924597263334, acc 54.0\n",
      "user 9, loss 1.3609336078166965, acc 51.2\n",
      "multiKrum Selected idxs: [9 3 1 4 0 6 2 5 7]\n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training Loss : 1.6026036010032423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:51<10:33, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 49.61% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 3.27, confidence: 0.13261533975601197\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 4 |\n",
      "=========================================\n",
      "user 0, loss 1.379558191793721, acc 41.0, mal loss 0.06548240035772324, mal acc 100.0\n",
      "user 1, loss 1.2321294536193212, acc 53.6\n",
      "user 2, loss 1.2193913544217747, acc 54.6\n",
      "user 3, loss 1.2300365681449572, acc 53.800000000000004\n",
      "user 4, loss 1.2192547385891277, acc 54.2\n",
      "user 5, loss 1.2370864232381187, acc 56.00000000000001\n",
      "user 6, loss 1.2208903491497038, acc 53.400000000000006\n",
      "user 7, loss 1.1912505875031154, acc 55.2\n",
      "user 8, loss 1.258534910281499, acc 55.400000000000006\n",
      "user 9, loss 1.2421248247226078, acc 53.2\n",
      "multiKrum Selected idxs: [3 9 1 4 0 6 2 5 7]\n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training Loss : 1.5127091357890303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [01:08<10:10, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 50.58% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 3.07, confidence: 0.15921896696090698\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 5 |\n",
      "=========================================\n",
      "user 0, loss 1.1617770108992491, acc 42.0, mal loss 0.023062309250235558, mal acc 100.0\n",
      "user 1, loss 1.1423192779223124, acc 61.199999999999996\n",
      "user 2, loss 1.139873133599758, acc 58.4\n",
      "user 3, loss 1.135922501484553, acc 55.400000000000006\n",
      "user 4, loss 1.1170293991764388, acc 56.599999999999994\n",
      "user 5, loss 1.1621691331267356, acc 56.8\n",
      "user 6, loss 1.13108346760273, acc 56.99999999999999\n",
      "user 7, loss 1.1154400850335755, acc 61.6\n",
      "user 8, loss 1.1729131708542504, acc 57.4\n",
      "user 9, loss 1.1435633411010107, acc 58.4\n",
      "multiKrum Selected idxs: [3 9 1 6 4 0 2 5 7]\n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training Loss : 1.4386091190472365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [01:25<09:54, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 56.13% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 3.62, confidence: 0.08346236944198608\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 6 |\n",
      "=========================================\n",
      "user 0, loss 1.0271850798915072, acc 42.4, mal loss 0.0023041735403239727, mal acc 100.0\n",
      "user 1, loss 1.0586956545710564, acc 62.2\n",
      "user 2, loss 1.046783257027467, acc 58.4\n",
      "user 3, loss 1.0592280566692351, acc 59.4\n",
      "user 4, loss 1.0393342489997546, acc 57.99999999999999\n",
      "user 5, loss 1.0773220658302307, acc 60.8\n",
      "user 6, loss 1.0617505038777986, acc 61.6\n",
      "user 7, loss 1.0238531822959582, acc 63.4\n",
      "user 8, loss 1.1016332377990086, acc 59.8\n",
      "user 9, loss 1.0726208314299583, acc 58.599999999999994\n",
      "multiKrum Selected idxs: [3 1 9 4 6 0 2 5 7]\n",
      " \n",
      "Avg Training Stats after 6 global rounds:\n",
      "Training Loss : 1.3749810345125635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [01:41<09:35, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 58.08% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.21, confidence: 0.050873613357543944\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 7 |\n",
      "=========================================\n",
      "user 0, loss 0.9037584980450025, acc 45.2, mal loss 0.00042441641562618315, mal acc 100.0\n",
      "user 1, loss 1.0103295584519705, acc 61.6\n",
      "user 2, loss 1.0022514869769414, acc 62.6\n",
      "user 3, loss 1.0162510335445403, acc 60.199999999999996\n",
      "user 4, loss 0.9836783582965533, acc 59.199999999999996\n",
      "user 5, loss 1.0220541020234426, acc 59.4\n",
      "user 6, loss 0.9971880257129668, acc 61.199999999999996\n",
      "user 7, loss 0.969988603889942, acc 63.800000000000004\n",
      "user 8, loss 1.0468553513288497, acc 61.4\n",
      "user 9, loss 1.0321222737431526, acc 63.6\n",
      "multiKrum Selected idxs: [3 1 9 4 0 6 2 5 7]\n",
      " \n",
      "Avg Training Stats after 7 global rounds:\n",
      "Training Loss : 1.3211905623252453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [01:59<09:20, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.63% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.94, confidence: 0.019955503940582275\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 8 |\n",
      "=========================================\n",
      "user 0, loss 0.802704358030474, acc 48.6, mal loss 0.0001607033482287079, mal acc 100.0\n",
      "user 1, loss 0.9597320755322775, acc 66.8\n",
      "user 2, loss 0.9776947438716888, acc 60.4\n",
      "user 3, loss 0.9737616633375485, acc 57.4\n",
      "user 4, loss 0.9527640109260878, acc 61.4\n",
      "user 5, loss 0.986908561984698, acc 63.2\n",
      "user 6, loss 0.9688014686107635, acc 65.2\n",
      "user 7, loss 0.9366318687796592, acc 63.6\n",
      "user 8, loss 1.013005857169628, acc 58.8\n",
      "user 9, loss 1.007703929642836, acc 62.2\n",
      "multiKrum Selected idxs: [3 1 4 9 0 6 2 5 7]\n",
      " \n",
      "Avg Training Stats after 8 global rounds:\n",
      "Training Loss : 1.2757880987581602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [02:16<09:06, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.64% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.70, confidence: 0.009934159368276596\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 9 |\n",
      "=========================================\n",
      "user 0, loss 0.7271867977735414, acc 48.8, mal loss 6.677482451777905e-05, mal acc 100.0\n",
      "user 1, loss 0.9319433574875197, acc 64.2\n",
      "user 2, loss 0.9261750499407451, acc 63.0\n",
      "user 3, loss 0.9587586502234142, acc 59.599999999999994\n",
      "user 4, loss 0.921328502893448, acc 64.0\n",
      "user 5, loss 0.9484923750162125, acc 63.4\n",
      "user 6, loss 0.9261176288127899, acc 63.4\n",
      "user 7, loss 0.8986759672562282, acc 62.6\n",
      "user 8, loss 0.9843053812781969, acc 61.0\n",
      "user 9, loss 0.9430808146794639, acc 63.0\n",
      "multiKrum Selected idxs: [1 3 6 4 9 5 0 2 7]\n",
      " \n",
      "Avg Training Stats after 9 global rounds:\n",
      "Training Loss : 1.2358790269557154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [02:32<08:38, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.65% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.56, confidence: 0.00950404554605484\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 10 |\n",
      "=========================================\n",
      "user 0, loss 0.7003296666188975, acc 47.0, mal loss 7.187772280303761e-05, mal acc 100.0\n",
      "user 1, loss 0.903691573937734, acc 65.8\n",
      "user 2, loss 0.92502205123504, acc 62.6\n",
      "user 3, loss 0.9317049846053124, acc 60.0\n",
      "user 4, loss 0.89521857102712, acc 61.4\n",
      "user 5, loss 0.9318300073345503, acc 65.4\n",
      "user 6, loss 0.9167026927073797, acc 65.8\n",
      "user 7, loss 0.8710395942131678, acc 67.4\n",
      "user 8, loss 0.9725919817884763, acc 62.8\n",
      "user 9, loss 0.9330906232198078, acc 60.8\n",
      "multiKrum Selected idxs: [1 3 6 4 9 0 5 2 7]\n",
      " \n",
      "Avg Training Stats after 10 global rounds:\n",
      "Training Loss : 1.2021033417270186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [02:48<08:14, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.71% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 6.76, confidence: 0.0037828363478183747\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 11 |\n",
      "=========================================\n",
      "user 0, loss 0.703242799929699, acc 48.8, mal loss 6.586923700524494e-05, mal acc 100.0\n",
      "user 1, loss 0.9083279967308044, acc 65.2\n",
      "user 2, loss 0.9154629970590275, acc 62.2\n",
      "user 3, loss 0.9166993459065754, acc 61.0\n",
      "user 4, loss 0.8977939143776893, acc 63.0\n",
      "user 5, loss 0.9128001103798549, acc 64.8\n",
      "user 6, loss 0.8996782233317693, acc 63.0\n",
      "user 7, loss 0.8720411732792854, acc 67.0\n",
      "user 8, loss 0.9849883243441582, acc 57.4\n",
      "user 9, loss 0.9246296803156535, acc 62.2\n",
      "multiKrum Selected idxs: [1 3 6 4 0 5 9 2 7]\n",
      " \n",
      "Avg Training Stats after 11 global rounds:\n",
      "Training Loss : 1.1740545339850579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [03:03<07:51, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.56% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.60, confidence: 0.017185431718826295\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 12 |\n",
      "=========================================\n",
      "user 0, loss 0.6765836923907518, acc 48.4, mal loss 0.0001330676896031946, mal acc 100.0\n",
      "user 1, loss 0.8878487512469292, acc 65.4\n",
      "user 2, loss 0.8856975197792053, acc 60.0\n",
      "user 3, loss 0.8969575037558873, acc 60.6\n",
      "user 4, loss 0.8828225741783777, acc 63.0\n",
      "user 5, loss 0.9007514576117197, acc 66.0\n",
      "user 6, loss 0.8848505213856698, acc 62.8\n",
      "user 7, loss 0.841272000471751, acc 67.0\n",
      "user 8, loss 0.9704656784733136, acc 60.0\n",
      "user 9, loss 0.9109589939316113, acc 59.0\n",
      "multiKrum Selected idxs: [1 3 6 4 0 5 2 7 9]\n",
      " \n",
      "Avg Training Stats after 12 global rounds:\n",
      "Training Loss : 1.1490350619298466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [03:20<07:39, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.53% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.40, confidence: 0.011097808927297592\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 13 |\n",
      "=========================================\n",
      "user 0, loss 0.6694362370602075, acc 36.8, mal loss 0.00019284537120256573, mal acc 100.0\n",
      "user 1, loss 0.8714123542110125, acc 66.4\n",
      "user 2, loss 0.8850762829184532, acc 63.0\n",
      "user 3, loss 0.8999466647704443, acc 61.199999999999996\n",
      "user 4, loss 0.8539220010240873, acc 64.4\n",
      "user 5, loss 0.9012362211942673, acc 65.4\n",
      "user 6, loss 0.8869411354263623, acc 64.8\n",
      "user 7, loss 0.8398581979175409, acc 63.800000000000004\n",
      "user 8, loss 0.9631072819232941, acc 61.0\n",
      "user 9, loss 0.9057007173697155, acc 64.0\n",
      "multiKrum Selected idxs: [1 3 6 4 0 2 5 7 8]\n",
      " \n",
      "Avg Training Stats after 13 global rounds:\n",
      "Training Loss : 1.127391111733823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [03:37<07:27, 16.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 59.10% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.46, confidence: 0.014813774824142456\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 14 |\n",
      "=========================================\n",
      "user 0, loss 0.7206156141249126, acc 46.400000000000006, mal loss 6.482137541752309e-05, mal acc 100.0\n",
      "user 1, loss 0.8751811782519022, acc 65.2\n",
      "user 2, loss 0.8920989980300268, acc 61.8\n",
      "user 3, loss 0.8906391218304633, acc 62.0\n",
      "user 4, loss 0.8708414882421494, acc 64.0\n",
      "user 5, loss 0.9065924232204754, acc 66.4\n",
      "user 6, loss 0.888652307788531, acc 64.0\n",
      "user 7, loss 0.8369272689024608, acc 65.60000000000001\n",
      "user 8, loss 0.9405891021092732, acc 62.4\n",
      "user 9, loss 0.9430314376950264, acc 65.2\n",
      "multiKrum Selected idxs: [1 3 6 4 0 2 7 5 8]\n",
      " \n",
      "Avg Training Stats after 14 global rounds:\n",
      "Training Loss : 1.10947152475423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [03:54<07:14, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.21% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.67, confidence: 0.009616123139858246\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 15 |\n",
      "=========================================\n",
      "user 0, loss 0.7644018292659184, acc 50.2, mal loss 6.796786328777671e-05, mal acc 100.0\n",
      "user 1, loss 0.8563734635710717, acc 66.0\n",
      "user 2, loss 0.8683512846628826, acc 61.4\n",
      "user 3, loss 0.8841110537449519, acc 58.599999999999994\n",
      "user 4, loss 0.8625182305773099, acc 63.800000000000004\n",
      "user 5, loss 0.8858868623773257, acc 66.60000000000001\n",
      "user 6, loss 0.8860946416854859, acc 62.8\n",
      "user 7, loss 0.8475749149918558, acc 66.2\n",
      "user 8, loss 0.9149371147155761, acc 62.8\n",
      "user 9, loss 0.9345326706767082, acc 64.2\n",
      "multiKrum Selected idxs: [6 1 3 4 2 0 5 7 8]\n",
      " \n",
      "Avg Training Stats after 15 global rounds:\n",
      "Training Loss : 1.0935386368790752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [04:11<06:59, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.51% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 7.78, confidence: 0.0019176002591848374\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 16 |\n",
      "=========================================\n",
      "user 0, loss 0.6646458865630789, acc 35.199999999999996, mal loss 0.00013130353181622922, mal acc 100.0\n",
      "user 1, loss 0.8666379332542419, acc 66.2\n",
      "user 2, loss 0.862246193488439, acc 61.199999999999996\n",
      "user 3, loss 0.8624662791689236, acc 61.4\n",
      "user 4, loss 0.8404543682932853, acc 63.800000000000004\n",
      "user 5, loss 0.8734490156173705, acc 65.60000000000001\n",
      "user 6, loss 0.8703455915053685, acc 63.2\n",
      "user 7, loss 0.8289891307552656, acc 65.60000000000001\n",
      "user 8, loss 0.8964705655972164, acc 59.4\n",
      "user 9, loss 0.9307398214936256, acc 64.8\n",
      "multiKrum Selected idxs: [6 1 3 4 0 2 7 5 8]\n",
      " \n",
      "Avg Training Stats after 16 global rounds:\n",
      "Training Loss : 1.0782952519849882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [04:28<06:46, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 57.74% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.32, confidence: 0.07054187059402466\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 17 |\n",
      "=========================================\n",
      "user 0, loss 0.7362753026841902, acc 48.199999999999996, mal loss 0.00013495182793121785, mal acc 100.0\n",
      "user 1, loss 0.8699577579895655, acc 65.60000000000001\n",
      "user 2, loss 0.8616003503402073, acc 61.4\n",
      "user 3, loss 0.8682055766383807, acc 61.8\n",
      "user 4, loss 0.8460817332069079, acc 67.2\n",
      "user 5, loss 0.879853792488575, acc 66.60000000000001\n",
      "user 6, loss 0.8586840654412905, acc 64.4\n",
      "user 7, loss 0.8319392507274945, acc 66.2\n",
      "user 8, loss 0.8863840882976849, acc 63.2\n",
      "user 9, loss 0.9358140021562575, acc 64.2\n",
      "multiKrum Selected idxs: [3 6 1 4 0 2 7 8 5]\n",
      " \n",
      "Avg Training Stats after 17 global rounds:\n",
      "Training Loss : 1.06530609551511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [04:45<06:27, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 62.13% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.26, confidence: 0.016662678122520445\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 18 |\n",
      "=========================================\n",
      "user 0, loss 0.6893891380750574, acc 32.4, mal loss 0.000163293065270409, mal acc 100.0\n",
      "user 1, loss 0.8471390028794605, acc 63.4\n",
      "user 2, loss 0.8488164469599724, acc 59.8\n",
      "user 3, loss 0.865122743944327, acc 60.8\n",
      "user 4, loss 0.8375808584193388, acc 64.8\n",
      "user 5, loss 0.8638080755869547, acc 67.60000000000001\n",
      "user 6, loss 0.8468566457430522, acc 63.0\n",
      "user 7, loss 0.811682175596555, acc 66.0\n",
      "user 8, loss 0.8838070819775264, acc 60.4\n",
      "user 9, loss 0.944832555949688, acc 64.4\n",
      "multiKrum Selected idxs: [4 3 6 1 0 2 7 8 5]\n",
      " \n",
      "Avg Training Stats after 18 global rounds:\n",
      "Training Loss : 1.053005949792781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [05:02<06:10, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 59.25% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.64, confidence: 0.025417262315750123\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 19 |\n",
      "=========================================\n",
      "user 0, loss 0.736417043901605, acc 40.0, mal loss 0.00011570921924430877, mal acc 100.0\n",
      "user 1, loss 0.8510081032911936, acc 65.8\n",
      "user 2, loss 0.8555316612124443, acc 63.6\n",
      "user 3, loss 0.8675304909547169, acc 62.0\n",
      "user 4, loss 0.8336442043383916, acc 66.0\n",
      "user 5, loss 0.8860998859008152, acc 66.0\n",
      "user 6, loss 0.8448133622606596, acc 64.60000000000001\n",
      "user 7, loss 0.8241369421283404, acc 65.60000000000001\n",
      "user 8, loss 0.88052991827329, acc 62.4\n",
      "user 9, loss 0.9329145352045695, acc 61.8\n",
      "multiKrum Selected idxs: [4 1 6 3 2 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 19 global rounds:\n",
      "Training Loss : 1.0423878795271928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [05:19<05:54, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 62.99% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.79, confidence: 0.00982079803943634\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 20 |\n",
      "=========================================\n",
      "user 0, loss 0.7411202593622312, acc 50.8, mal loss 5.495176446856931e-05, mal acc 100.0\n",
      "user 1, loss 0.8575156678756078, acc 66.4\n",
      "user 2, loss 0.8563144321242969, acc 63.800000000000004\n",
      "user 3, loss 0.856264683107535, acc 61.8\n",
      "user 4, loss 0.8390387463072937, acc 66.2\n",
      "user 5, loss 0.8739453434944154, acc 66.0\n",
      "user 6, loss 0.8553252905607224, acc 66.0\n",
      "user 7, loss 0.8115133066972097, acc 64.2\n",
      "user 8, loss 0.8980043192704518, acc 63.4\n",
      "user 9, loss 0.9361962253848711, acc 65.4\n",
      "multiKrum Selected idxs: [4 6 1 3 2 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 20 global rounds:\n",
      "Training Loss : 1.0328946769217562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [05:36<05:38, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.30% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.07, confidence: 0.01887124329805374\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 21 |\n",
      "=========================================\n",
      "user 0, loss 0.7065348199817608, acc 40.6, mal loss 0.00016802878235466778, mal acc 100.0\n",
      "user 1, loss 0.8379744360844295, acc 65.60000000000001\n",
      "user 2, loss 0.8470215444763501, acc 62.8\n",
      "user 3, loss 0.846547537545363, acc 63.4\n",
      "user 4, loss 0.8550084287921588, acc 63.2\n",
      "user 5, loss 0.872753884891669, acc 65.0\n",
      "user 6, loss 0.8452354873220127, acc 59.199999999999996\n",
      "user 7, loss 0.8118019858996073, acc 65.2\n",
      "user 8, loss 0.8867517034212749, acc 64.8\n",
      "user 9, loss 0.9394607072075208, acc 63.2\n",
      "multiKrum Selected idxs: [1 6 4 3 2 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 21 global rounds:\n",
      "Training Loss : 1.0239429805713018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [05:53<05:21, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 62.15% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.35, confidence: 0.01800987869501114\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 22 |\n",
      "=========================================\n",
      "user 0, loss 0.680108409802045, acc 46.2, mal loss 0.00019100538338534534, mal acc 100.0\n",
      "user 1, loss 0.8415547912319501, acc 64.0\n",
      "user 2, loss 0.8426119153698286, acc 63.0\n",
      "user 3, loss 0.8429722492893537, acc 58.599999999999994\n",
      "user 4, loss 0.8181032379468283, acc 66.2\n",
      "user 5, loss 0.8631352076927822, acc 65.60000000000001\n",
      "user 6, loss 0.8379638741413752, acc 66.2\n",
      "user 7, loss 0.7974479285379251, acc 65.0\n",
      "user 8, loss 0.8723624885082245, acc 65.60000000000001\n",
      "user 9, loss 0.9370787387092908, acc 64.0\n",
      "multiKrum Selected idxs: [1 6 4 2 3 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 22 global rounds:\n",
      "Training Loss : 1.015278930732741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [06:09<05:01, 16.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.62% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.37, confidence: 0.011421616375446319\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 23 |\n",
      "=========================================\n",
      "user 0, loss 0.7087794390556259, acc 43.8, mal loss 0.00015937790158204734, mal acc 100.0\n",
      "user 1, loss 0.8371453933417797, acc 63.6\n",
      "user 2, loss 0.8445590456326801, acc 65.0\n",
      "user 3, loss 0.8556472117702166, acc 63.800000000000004\n",
      "user 4, loss 0.8154093896349272, acc 63.0\n",
      "user 5, loss 0.8561044464508694, acc 64.0\n",
      "user 6, loss 0.8510754490892092, acc 64.8\n",
      "user 7, loss 0.8056506882111232, acc 66.0\n",
      "user 8, loss 0.870650348563989, acc 63.800000000000004\n",
      "user 9, loss 0.9357206940650941, acc 64.4\n",
      "multiKrum Selected idxs: [1 6 4 2 3 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 23 global rounds:\n",
      "Training Loss : 1.0075743776826893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [06:26<04:45, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 62.90% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.44, confidence: 0.040056297183036806\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 24 |\n",
      "=========================================\n",
      "user 0, loss 0.7390010819646883, acc 39.4, mal loss 0.000151561907841824, mal acc 100.0\n",
      "user 1, loss 0.8596868654092154, acc 63.6\n",
      "user 2, loss 0.8468176563580831, acc 62.0\n",
      "user 3, loss 0.8431621039907137, acc 60.6\n",
      "user 4, loss 0.8112995892763138, acc 64.8\n",
      "user 5, loss 0.8587125316262245, acc 66.4\n",
      "user 6, loss 0.856001627445221, acc 65.0\n",
      "user 7, loss 0.8135024483005205, acc 67.0\n",
      "user 8, loss 0.8687902341286341, acc 65.2\n",
      "user 9, loss 0.9394283051292099, acc 64.0\n",
      "multiKrum Selected idxs: [1 6 2 4 3 0 7 8 5]\n",
      " \n",
      "Avg Training Stats after 24 global rounds:\n",
      "Training Loss : 1.000743788794364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [06:43<04:30, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.77% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.19, confidence: 0.016384586691856384\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 25 |\n",
      "=========================================\n",
      "user 0, loss 0.6888893188338923, acc 46.6, mal loss 0.0001226224994752556, mal acc 100.0\n",
      "user 1, loss 0.8522674252589543, acc 64.2\n",
      "user 2, loss 0.8555524587631226, acc 63.2\n",
      "user 3, loss 0.8362856244047482, acc 62.2\n",
      "user 4, loss 0.8363613133629163, acc 62.8\n",
      "user 5, loss 0.8525367637475331, acc 63.4\n",
      "user 6, loss 0.8590433403849601, acc 61.6\n",
      "user 7, loss 0.8024656573931376, acc 64.4\n",
      "user 8, loss 0.8650992294152577, acc 62.8\n",
      "user 9, loss 0.9342411230007807, acc 65.8\n",
      "multiKrum Selected idxs: [1 6 0 2 4 7 3 8 5]\n",
      " \n",
      "Avg Training Stats after 25 global rounds:\n",
      "Training Loss : 0.9942450062608507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [07:00<04:13, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 64.65% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.25, confidence: 0.019903647899627685\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 26 |\n",
      "=========================================\n",
      "user 0, loss 0.7209574990532914, acc 32.6, mal loss 0.0001615207438590005, mal acc 100.0\n",
      "user 1, loss 0.8525873571634293, acc 65.2\n",
      "user 2, loss 0.8434572214881579, acc 64.0\n",
      "user 3, loss 0.8305199508865675, acc 63.800000000000004\n",
      "user 4, loss 0.824995699028174, acc 64.4\n",
      "user 5, loss 0.8502351524929206, acc 65.60000000000001\n",
      "user 6, loss 0.8447514976064364, acc 65.60000000000001\n",
      "user 7, loss 0.8034459449350834, acc 63.800000000000004\n",
      "user 8, loss 0.8528722827633222, acc 64.0\n",
      "user 9, loss 0.9284255877137184, acc 66.0\n",
      "multiKrum Selected idxs: [6 1 2 0 8 7 3 4 5]\n",
      " \n",
      "Avg Training Stats after 26 global rounds:\n",
      "Training Loss : 0.9881288452243991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [07:17<03:58, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 58.17% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.49, confidence: 0.030006948113441467\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 27 |\n",
      "=========================================\n",
      "user 0, loss 0.7383905275161918, acc 52.0, mal loss 5.9290556237101555e-05, mal acc 100.0\n",
      "user 1, loss 0.8475641975800197, acc 62.6\n",
      "user 2, loss 0.8335680782794953, acc 65.60000000000001\n",
      "user 3, loss 0.8348549246788025, acc 60.199999999999996\n",
      "user 4, loss 0.8301089790960153, acc 64.8\n",
      "user 5, loss 0.8485307415326436, acc 65.8\n",
      "user 6, loss 0.8517468313376108, acc 64.60000000000001\n",
      "user 7, loss 0.8058320365846158, acc 64.60000000000001\n",
      "user 8, loss 0.8578083897630374, acc 63.4\n",
      "user 9, loss 0.9424886112411817, acc 62.2\n",
      "multiKrum Selected idxs: [1 6 7 0 8 2 3 4 5]\n",
      " \n",
      "Avg Training Stats after 27 global rounds:\n",
      "Training Loss : 0.9826088632442718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [07:34<03:41, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 65.57% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 7.62, confidence: 0.002195281721651554\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 28 |\n",
      "=========================================\n",
      "user 0, loss 0.7314519626762798, acc 43.0, mal loss 0.00019348814385011792, mal acc 100.0\n",
      "user 1, loss 0.8289011726776758, acc 65.60000000000001\n",
      "user 2, loss 0.8241460387905439, acc 64.4\n",
      "user 3, loss 0.8342846338947614, acc 64.2\n",
      "user 4, loss 0.8201190071801344, acc 66.2\n",
      "user 5, loss 0.8324490194519361, acc 65.2\n",
      "user 6, loss 0.8513834921022255, acc 63.4\n",
      "user 7, loss 0.7961581585307916, acc 64.2\n",
      "user 8, loss 0.8525669405857722, acc 63.6\n",
      "user 9, loss 0.9403883839646975, acc 65.8\n",
      "multiKrum Selected idxs: [1 6 2 7 0 8 3 4 5]\n",
      " \n",
      "Avg Training Stats after 28 global rounds:\n",
      "Training Loss : 0.9772008638778864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [07:51<03:22, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.35% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.10, confidence: 0.05585706830024719\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 29 |\n",
      "=========================================\n",
      "user 0, loss 0.6796735664245169, acc 44.2, mal loss 0.0002618308353703469, mal acc 100.0\n",
      "user 1, loss 0.8255646154284477, acc 64.60000000000001\n",
      "user 2, loss 0.834193276365598, acc 67.60000000000001\n",
      "user 3, loss 0.8261335591475168, acc 64.60000000000001\n",
      "user 4, loss 0.8185216213266054, acc 64.2\n",
      "user 5, loss 0.8513385370373725, acc 66.60000000000001\n",
      "user 6, loss 0.8431899830698967, acc 64.4\n",
      "user 7, loss 0.8146077225605648, acc 64.60000000000001\n",
      "user 8, loss 0.8424055655797322, acc 66.4\n",
      "user 9, loss 0.9119905347625415, acc 64.0\n",
      "multiKrum Selected idxs: [1 6 7 8 2 0 3 4 5]\n",
      " \n",
      "Avg Training Stats after 29 global rounds:\n",
      "Training Loss : 0.9719443478190034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [08:08<03:05, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.57% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 6.29, confidence: 0.007294858992099762\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 30 |\n",
      "=========================================\n",
      "user 0, loss 0.7090674843842626, acc 37.4, mal loss 0.00023374814190901816, mal acc 100.0\n",
      "user 1, loss 0.8266932951907316, acc 65.4\n",
      "user 2, loss 0.822452970345815, acc 64.60000000000001\n",
      "user 3, loss 0.8185045100748539, acc 63.0\n",
      "user 4, loss 0.8054686983426412, acc 67.4\n",
      "user 5, loss 0.8304613093535106, acc 65.60000000000001\n",
      "user 6, loss 0.8367633163928986, acc 65.2\n",
      "user 7, loss 0.8006918400526047, acc 65.8\n",
      "user 8, loss 0.8605601921677589, acc 65.8\n",
      "user 9, loss 0.9245415478944778, acc 62.2\n",
      "multiKrum Selected idxs: [1 0 6 7 2 8 3 4 5]\n",
      " \n",
      "Avg Training Stats after 30 global rounds:\n",
      "Training Loss : 0.9669968867723685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [08:24<02:48, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.60% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.35, confidence: 0.02991083264350891\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 31 |\n",
      "=========================================\n",
      "user 0, loss 0.7155614157263624, acc 45.4, mal loss 0.00011647583596641198, mal acc 100.0\n",
      "user 1, loss 0.8407783980170885, acc 64.8\n",
      "user 2, loss 0.8227474679549536, acc 64.60000000000001\n",
      "user 3, loss 0.8238016640146574, acc 63.6\n",
      "user 4, loss 0.8175693800052007, acc 67.60000000000001\n",
      "user 5, loss 0.8292450860142707, acc 66.2\n",
      "user 6, loss 0.833094446361065, acc 64.60000000000001\n",
      "user 7, loss 0.7857264548540116, acc 66.60000000000001\n",
      "user 8, loss 0.8376544843117396, acc 62.0\n",
      "user 9, loss 0.9245383600393932, acc 61.8\n",
      "multiKrum Selected idxs: [1 6 7 0 8 2 3 4 5]\n",
      " \n",
      "Avg Training Stats after 31 global rounds:\n",
      "Training Loss : 0.9623541393193847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [08:42<02:31, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 63.14% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 6.05, confidence: 0.010706515610218048\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 32 |\n",
      "=========================================\n",
      "user 0, loss 0.7037208263366672, acc 37.2, mal loss 0.00014166624168865383, mal acc 100.0\n",
      "user 1, loss 0.8370699981848398, acc 65.0\n",
      "user 2, loss 0.831674196322759, acc 64.8\n",
      "user 3, loss 0.8092826535304387, acc 63.800000000000004\n",
      "user 4, loss 0.8026244461536406, acc 66.0\n",
      "user 5, loss 0.8134928668538729, acc 66.60000000000001\n",
      "user 6, loss 0.8368791555364927, acc 66.0\n",
      "user 7, loss 0.7930089498559635, acc 67.60000000000001\n",
      "user 8, loss 0.8326300889253616, acc 65.4\n",
      "user 9, loss 0.9197894593079884, acc 66.0\n",
      "multiKrum Selected idxs: [1 6 8 7 0 2 3 4 5]\n",
      " \n",
      "Avg Training Stats after 32 global rounds:\n",
      "Training Loss : 0.9578436119688041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [08:58<02:14, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.03% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.32, confidence: 0.015716350078582762\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 33 |\n",
      "=========================================\n",
      "user 0, loss 0.7056989094670256, acc 38.4, mal loss 0.00011981597344856709, mal acc 100.0\n",
      "user 1, loss 0.8283940955996513, acc 67.80000000000001\n",
      "user 2, loss 0.8279005388418833, acc 65.2\n",
      "user 3, loss 0.8215995642046133, acc 63.2\n",
      "user 4, loss 0.8391515408953031, acc 66.4\n",
      "user 5, loss 0.8058963929613432, acc 64.0\n",
      "user 6, loss 0.835112046202024, acc 63.800000000000004\n",
      "user 7, loss 0.799680066605409, acc 66.4\n",
      "user 8, loss 0.8381292084852854, acc 66.2\n",
      "user 9, loss 0.9159354403614998, acc 64.8\n",
      "multiKrum Selected idxs: [1 7 8 0 6 2 3 4 5]\n",
      " \n",
      "Avg Training Stats after 33 global rounds:\n",
      "Training Loss : 0.9537195564655798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [09:15<01:56, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 61.98% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.50, confidence: 0.03947035372257233\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 34 |\n",
      "=========================================\n",
      "user 0, loss 0.6735875920091597, acc 33.2, mal loss 0.0001561870740260929, mal acc 100.0\n",
      "user 1, loss 0.8203843275705974, acc 66.2\n",
      "user 2, loss 0.8227521901329359, acc 64.8\n",
      "user 3, loss 0.8137192770838736, acc 60.199999999999996\n",
      "user 4, loss 0.8221065829197566, acc 65.60000000000001\n",
      "user 5, loss 0.8163483401139576, acc 65.4\n",
      "user 6, loss 0.8426359365383784, acc 65.60000000000001\n",
      "user 7, loss 0.8007145285606384, acc 62.6\n",
      "user 8, loss 0.8309930359323819, acc 64.0\n",
      "user 9, loss 0.9095681702097257, acc 65.0\n",
      "multiKrum Selected idxs: [7 8 0 1 6 2 3 4 5]\n",
      " \n",
      "Avg Training Stats after 34 global rounds:\n",
      "Training Loss : 0.9496478341609198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [09:31<01:40, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.31% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.02, confidence: 0.03135620653629303\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 35 |\n",
      "=========================================\n",
      "user 0, loss 0.6819082957792296, acc 46.400000000000006, mal loss 0.00016283940931316465, mal acc 100.0\n",
      "user 1, loss 0.8182711084683736, acc 65.60000000000001\n",
      "user 2, loss 0.8318725536266962, acc 66.8\n",
      "user 3, loss 0.8028939910233022, acc 65.0\n",
      "user 4, loss 0.8119207347432772, acc 66.4\n",
      "user 5, loss 0.8100046170254549, acc 67.80000000000001\n",
      "user 6, loss 0.834604749083519, acc 64.2\n",
      "user 7, loss 0.7972229853272438, acc 63.800000000000004\n",
      "user 8, loss 0.8244703397154809, acc 63.6\n",
      "user 9, loss 0.9313903912901877, acc 63.800000000000004\n",
      "multiKrum Selected idxs: [1 7 0 8 2 6 3 4 5]\n",
      " \n",
      "Avg Training Stats after 35 global rounds:\n",
      "Training Loss : 0.9457852096594158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [09:48<01:24, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 64.71% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 7.60, confidence: 0.0028702694922685622\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 36 |\n",
      "=========================================\n",
      "user 0, loss 0.6664789981656893, acc 30.2, mal loss 0.00033181256731040776, mal acc 100.0\n",
      "user 1, loss 0.8165853266914684, acc 64.4\n",
      "user 2, loss 0.8162854572137196, acc 65.4\n",
      "user 3, loss 0.7999039724469185, acc 64.60000000000001\n",
      "user 4, loss 0.7921515340606371, acc 62.0\n",
      "user 5, loss 0.8188381681839626, acc 67.4\n",
      "user 6, loss 0.8256178632378578, acc 63.4\n",
      "user 7, loss 0.7971793442964555, acc 68.2\n",
      "user 8, loss 0.8121838599443435, acc 63.4\n",
      "user 9, loss 0.9176703085501989, acc 64.0\n",
      "multiKrum Selected idxs: [1 7 0 8 6 2 3 5 4]\n",
      " \n",
      "Avg Training Stats after 36 global rounds:\n",
      "Training Loss : 0.9419103283710744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [10:05<01:07, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 55.18% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 3.51, confidence: 0.11122643947601318\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 37 |\n",
      "=========================================\n",
      "user 0, loss 0.6700943904778375, acc 48.199999999999996, mal loss 0.00012157995661254972, mal acc 100.0\n",
      "user 1, loss 0.8283742214242616, acc 65.0\n",
      "user 2, loss 0.825907094279925, acc 67.4\n",
      "user 3, loss 0.8050602823495865, acc 63.6\n",
      "user 4, loss 0.8183184452354908, acc 63.2\n",
      "user 5, loss 0.8319308772683144, acc 67.2\n",
      "user 6, loss 0.842121723294258, acc 65.60000000000001\n",
      "user 7, loss 0.7882895280917485, acc 66.60000000000001\n",
      "user 8, loss 0.8282598917682966, acc 65.60000000000001\n",
      "user 9, loss 0.9335129638512929, acc 66.4\n",
      "multiKrum Selected idxs: [7 1 2 6 8 0 3 5 4]\n",
      " \n",
      "Avg Training Stats after 37 global rounds:\n",
      "Training Loss : 0.9385394260314265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [10:22<00:50, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 64.89% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 8.22, confidence: 0.0018043488264083863\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 38 |\n",
      "=========================================\n",
      "user 0, loss 0.6924139588784494, acc 33.2, mal loss 0.00015812258061487228, mal acc 100.0\n",
      "user 1, loss 0.8089055940508842, acc 62.6\n",
      "user 2, loss 0.810314904153347, acc 64.8\n",
      "user 3, loss 0.7974952215949695, acc 64.60000000000001\n",
      "user 4, loss 0.8132616887489954, acc 66.2\n",
      "user 5, loss 0.8147040531039238, acc 66.60000000000001\n",
      "user 6, loss 0.8276207730174066, acc 63.800000000000004\n",
      "user 7, loss 0.7905072105427583, acc 67.2\n",
      "user 8, loss 0.8221585641304653, acc 61.8\n",
      "user 9, loss 0.9086569140354793, acc 66.0\n",
      "multiKrum Selected idxs: [1 7 0 8 6 2 3 5 4]\n",
      " \n",
      "Avg Training Stats after 38 global rounds:\n",
      "Training Loss : 0.9351200697733802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [10:39<00:33, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 58.31% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.21, confidence: 0.06892266273498535\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 39 |\n",
      "=========================================\n",
      "user 0, loss 0.6953140595919316, acc 29.4, mal loss 0.00024590949760749936, mal acc 100.0\n",
      "user 1, loss 0.7982690438628196, acc 65.60000000000001\n",
      "user 2, loss 0.8210036327441533, acc 65.0\n",
      "user 3, loss 0.8129176025589308, acc 63.6\n",
      "user 4, loss 0.8071393430233001, acc 67.0\n",
      "user 5, loss 0.8187174275517464, acc 66.4\n",
      "user 6, loss 0.8194311857223511, acc 62.0\n",
      "user 7, loss 0.7885383126636345, acc 65.8\n",
      "user 8, loss 0.8375118970870972, acc 63.800000000000004\n",
      "user 9, loss 0.9184992894530296, acc 65.0\n",
      "multiKrum Selected idxs: [1 7 8 0 6 2 3 5 4]\n",
      " \n",
      "Avg Training Stats after 39 global rounds:\n",
      "Training Loss : 0.9319563289952396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [10:55<00:16, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.11% \n",
      "Global model Malicious Accuracy: 20.00%, Malicious Loss: 3.90, confidence: 0.10908899307250977\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 40 |\n",
      "=========================================\n",
      "user 0, loss 0.6326491393762208, acc 35.0, mal loss 0.00023404145031236112, mal acc 100.0\n",
      "user 1, loss 0.8260469034314156, acc 64.2\n",
      "user 2, loss 0.8118746909002463, acc 66.4\n",
      "user 3, loss 0.8060519280532996, acc 64.2\n",
      "user 4, loss 0.8071271749834219, acc 63.0\n",
      "user 5, loss 0.8282963742812474, acc 67.4\n",
      "user 6, loss 0.8089746023217836, acc 63.2\n",
      "user 7, loss 0.7897441854079563, acc 65.8\n",
      "user 8, loss 0.822410961985588, acc 63.2\n",
      "user 9, loss 0.9104026526212693, acc 66.8\n",
      "multiKrum Selected idxs: [1 7 2 0 8 6 3 4 5]\n",
      " \n",
      "Avg Training Stats after 40 global rounds:\n",
      "Training Loss : 0.9287663673037645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [11:10<00:00, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 60.05% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.96, confidence: 0.03499559760093689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 40 global rounds of training:\n",
      "|---- Test Benign Accuracy: 60.05%\n",
      "|---- Test Malicious Accuracy: 0.00%, Malicious Loss: 4.96, confidence:0.03499559462070465\n",
      "\n",
      "\n",
      " Total Run Time: 673.1476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if args.gpu == 0 else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "        elif args.dataset == 'cifar100':\n",
    "            global_model = LeNet5(args=args)\n",
    "\n",
    "            \n",
    "    # elif args.model == 'alexnet':\n",
    "    #     if args.dataset == 'cifar100':\n",
    "    #         global_model = Alexnet(args=args)\n",
    "    \n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "        print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    \n",
    "    confidence = []    \n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "            \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_clients:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            else:\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            # boost the malicious (weight+bias) by alpha\n",
    "            # if mal_user:\n",
    "            #     flat_delta_m = flatten(w) - flatten(w_prev)\n",
    "            #     flat_mal_w = flatten(w_prev) + args.boost * flat_delta_m\n",
    "            #     w = construct_ordered_dict(global_model, torch.tensor(flat_mal_w).to(device))\n",
    "              \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "            \n",
    "            # if agg==krum, alternate training\n",
    "            if args.aggregation == 'krum':\n",
    "                if idx in args.mal_clients and epoch %2 ==0:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else:                     \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "            else:\n",
    "                # if malicious user: boost the detlta weights\n",
    "                if idx in args.mal_clients:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else: \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_clients)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif args.aggregation == 'fltrust':\n",
    "            glob_weights = []\n",
    "            glob_weights.append(flatten(global_weights))\n",
    "            agg_weights = fltrust(malicious_grads, glob_weights)\n",
    "\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]\n",
    "            agg_weights, selected_idxs = fed_cc(local_weights, glob_plr, 'kernel')\n",
    "            print(f'fed_cc Selected idx: {selected_idxs}')\n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "            \n",
    "            if len(args.mal_clients) > 0:\n",
    "                mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "                print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "                confidence.append(mal_out)\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    \n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "        print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out))\n",
    "    \n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    data = {\n",
    "        \"clean_acc\": test_acc,\n",
    "        \"backdoor_acc\": mal_acc,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "    csv_file_path = f\"output/confidence_{args.alpha}_{args.dataset}_{args.aggregation}.csv\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20bde6-2276-4d49-803f-2e64717e58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cifar trmean 1 fmnist trmean\n",
    "\n",
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 50\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 3 # num of local epoch\n",
    "    local_bs = 100 # batch size\n",
    "    lr = 0.005\n",
    "    momentum = 0.9\n",
    "    aggregation = 'mkrum' # fedavg, krum, mkrum, coomed, bulyan, flare, fedcc\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'cifar' # fmnist, cifar, mnist\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes = 100 \n",
    "        num_channels = 3 # num of channels of imgs\n",
    "    else:\n",
    "        num_classes = 10\n",
    "        num_channels = 1\n",
    "    \n",
    "    gpu = 0\n",
    "    optimizer = 'adam'\n",
    "    iid = 1 # 0 for non-iid\n",
    "    alpha = 1 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_clients = [0] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 5 # number of maliciuos data sample\n",
    "    mal_bs = 100\n",
    "    mal_lr = 0.005\n",
    "    mal_test_bs = 100\n",
    "    local_mal_ep = 6\n",
    "    boost = 5 # alpha: 2 for fedavg, 3.5 for krum\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if args.gpu == 0 else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "        elif args.dataset == 'cifar100':\n",
    "            global_model = LeNet5(args=args)\n",
    "\n",
    "            \n",
    "    # elif args.model == 'alexnet':\n",
    "    #     if args.dataset == 'cifar100':\n",
    "    #         global_model = Alexnet(args=args)\n",
    "    \n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "        print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    \n",
    "    confidence = []    \n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "            \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_clients:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            else:\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            # boost the malicious (weight+bias) by alpha\n",
    "            # if mal_user:\n",
    "            #     flat_delta_m = flatten(w) - flatten(w_prev)\n",
    "            #     flat_mal_w = flatten(w_prev) + args.boost * flat_delta_m\n",
    "            #     w = construct_ordered_dict(global_model, torch.tensor(flat_mal_w).to(device))\n",
    "              \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "            \n",
    "            # if agg==krum, alternate training\n",
    "            if args.aggregation == 'krum':\n",
    "                if idx in args.mal_clients and epoch %2 ==0:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else:                     \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "            else:\n",
    "                # if malicious user: boost the detlta weights\n",
    "                if idx in args.mal_clients:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else: \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_clients)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif args.aggregation == 'fltrust':\n",
    "            glob_weights = []\n",
    "            glob_weights.append(flatten(global_weights))\n",
    "            agg_weights = fltrust(malicious_grads, glob_weights)\n",
    "\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer].reshape((glob_plr.shape[0], glob_plr.shape[1]))) for each_local in structured_local_weights]\n",
    "            agg_weights, selected_idxs = fed_cc(local_weights, glob_plr, 'kernel')\n",
    "            print(f'fed_cc Selected idx: {selected_idxs}')\n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "            \n",
    "            if len(args.mal_clients) > 0:\n",
    "                mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "                print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "                confidence.append(mal_out)\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    \n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "        print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out))\n",
    "    \n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    data = {\n",
    "        \"clean_acc\": test_acc,\n",
    "        \"backdoor_acc\": mal_acc,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "    csv_file_path = f\"output/confidence_{args.alpha}_{args.dataset}_{args.aggregation}.csv\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeef1b3-2a10-4cbf-af1c-55bd83b7f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda3a20-2321-4637-912f-1cf8e2b6dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_weights = []\n",
    "for key in global_weights:\n",
    "    if 'weight' in key:\n",
    "        only_weights = np.append(only_weights, global_weights[key].detach().cpu().numpy())\n",
    "_, glob_svd, _, = np.linalg.svd(only_weights.reshape(1,-1))\n",
    "glob_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b17f74-9b4f-40eb-886f-1000bf52b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('euclidean')\n",
    "for i in range(len(plrs)):\n",
    "    val = euclidean_distances(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    # print(val)\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63086320-b1f9-46ff-aed0-34819c44ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, paired_cosine_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('cosine')\n",
    "for i in range(len(plrs)):\n",
    "    val = cosine_similarity(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d998913-cbd5-4897-92f2-3d5bb6661a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5079-8318-48dc-9c60-de72aeef88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv1_weights = np.array(conv1_weights)\n",
    "conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv2_weights = np.array(conv2_weights)\n",
    "conv3_weights = [np.array(w['conv3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv3_weights = np.array(conv3_weights)\n",
    "fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc1_weights = np.array(fc1_weights)\n",
    "fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc2_weights = np.array(fc2_weights)\n",
    "\n",
    "num_layers = 5\n",
    "\n",
    "_, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "_, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "_, s_layer_2, _, = np.linalg.svd(conv3_weights)\n",
    "_, s_layer_3, _, = np.linalg.svd(fc1_weights)\n",
    "_, s_layer_4, _, = np.linalg.svd(fc2_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae72f18-e639-4408-a741-95151b72caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d6ead-4de5-4907-b640-037f51e08da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in zip(np.arange(num_layers), [s_layer_0, s_layer_1, s_layer_2, s_layer_3, s_layer_4]):\n",
    "    plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "    for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "        plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "plt.xticks(np.arange(num_layers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd88e6-ccbc-463e-8502-8364fd57a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_data = linkage(conv1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer1')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e212a-0411-4bd5-a0c8-a8811f3b09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer2')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb776-55aa-47e1-a710-1b3801e338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv3_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer3')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d785aca-b39b-45d9-a812-406235b4ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: PLR')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3a20f-0f1a-45d8-8942-aa1916f9f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer5')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c78f9d-cc6c-459d-8758-2404a5d83454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e286-f726-4044-95fb-ec25b21ab4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead0e8a-557b-4b3e-8d97-af9370d31ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8437cfc-68ed-48fc-8d82-9fc9bdfb4288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f315-00f2-4d04-87de-8acc99e07788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66663c2d-4265-4388-ac5c-715ee748a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "# conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv1_weights = np.array(conv1_weights)\n",
    "# conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv2_weights = np.array(conv2_weights)\n",
    "# fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc1_weights = np.array(fc1_weights)\n",
    "# fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc2_weights = np.array(fc2_weights)\n",
    "# fc3_weights = [np.array(w['fc3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc3_weights = np.array(fc3_weights)\n",
    "\n",
    "# num_layers = 5\n",
    "\n",
    "# _, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "# _, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "# _, s_layer_2, _, = np.linalg.svd(fc1_weights)\n",
    "# _, s_layer_3, _, = np.linalg.svd(fc2_weights)\n",
    "# _, s_layer_4, _, = np.linalg.svd(fc3_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3e820-c2be-48cc-b2da-f05321c4162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for x, y in zip(np.arange(num_layers-1), [s_layer_0, s_layer_1, s_layer_3, s_layer_4]):\n",
    "#     plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "#     for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "#         plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "# plt.xticks(np.arange(num_layers))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506c011-6105-4c3e-8c2c-d49c25ba4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(args.mal_clients) > 0:\n",
    "#     import csv\n",
    "#     if args.iid:\n",
    "#         filename = 'Confidence_2IID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "#     else:\n",
    "#         filename = 'Confidence_2NIID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "\n",
    "#     with open(filename, mode='w', newline='') as csvfile:\n",
    "#         w = csv.writer(csvfile)\n",
    "#         w.writerows(map(lambda x: [x], confidence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdac6fe-33d4-483e-97a8-3c6492435ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mal_dataset_of_class\n",
    "\n",
    "test_mal_X_list, test_mal_Y, test_Y_true = get_mal_dataset_of_class(test_dataset, args.num_mal*100, Y_true, mal_Y)\n",
    "flat_test_mal_Y = np.hstack([y for y in test_mal_Y])\n",
    "mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, test_mal_X_list, flat_test_mal_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd663b-a50f-4009-b8bb-6d649683eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_acc, mal_loss, torch.mean(mal_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26323f6a-6e52-4e2e-901c-ffeb1c2d2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset, args.boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399b8e9-1931-4681-a07a-8e9e5a692ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_details(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4c375-638b-44ec-99b1-ee0de7e75a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # PLOTTING (optional)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.figure()\n",
    "plt.title('Training Loss vs Communication rounds')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))\n",
    "\n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651cfc9-6967-478d-b278-aaee461c2364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2720a8a-16bf-4733-88e9-2f4f0a8e5c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283281e9-6358-4189-bf2c-17f4a52756f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba0841-613b-4826-90ec-bad5a5f9aec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a32436-4e4d-43b7-9f4f-497f39f205f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47210ea-dfd2-4616-980b-12ab6fa0dd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176abbe-ce6f-4d1a-a47c-a128948ffd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c4e17-b37f-44de-a77f-af95a2501ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35679455-ea48-4bba-b3a0-6e31f9964e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ab91-e74e-42f3-8225-0aac2a69754d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8860e-4684-49b6-972a-3cef7ee2ffab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28555239-1922-4967-93d0-66e55487009c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4055285-e919-4161-ae66-aa457d0a71ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47107d03-f993-4ca0-bbdd-f31a2cfe60f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c10f3-2874-49e5-912f-9298f11845cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f21ba6-d800-4e99-9e35-3bf16a935a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f072d26-05ac-432c-b815-6632679937ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d880f-143c-4f29-8a53-07068356af81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c0618-d2d4-4936-85cb-3a3662ab68de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332983fb-514b-4edf-a5a4-4862a1690c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f376f-9683-4700-bf7c-9847c019d2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a818b79-09ef-4e2a-8d26-db835ac3c799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136e5de-3e2c-452d-9f8f-c73916acc07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed62019-417b-4a9a-8a32-815a8cb3a186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03466f-4f21-45a2-af5c-6ccd3a977178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc5467-8c35-4243-a53e-c15c6858adf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b66986-8fe5-4bfe-b829-2266d53d9940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d04bb-3561-46bf-8e80-68c543054770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876db2a-5db7-49b8-8f6e-5ba7522c0d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36218335-8be7-45fb-b653-8845fedd10b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254f091-518a-44e9-9719-1d5ebfd115b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21fd7d-8526-48e5-a858-bc93043540fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5be937-c11f-401f-85d0-af2ab9853b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc5d9c-83e5-4af2-aa3f-b1453283c5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1a7b5-01e3-4381-8121-e85c4d2a5b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e73d7-a0c1-4b2b-89db-c8dc04744d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b36d4-7947-4b81-862e-9d8998bee357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca6063-65e8-4cc4-9e35-a8c74f05cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732ee80-a461-4ad3-80cd-0db9e03929d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebd4b2-636e-4956-8871-052e18bf595a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2fef8f-e30e-460d-995b-7e51396e0e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fe000-7122-45f7-bd2a-9fe3566603d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1d92e-8fa7-4da5-b0c0-364dae884e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4e452-ed7c-41f0-8599-eb203f87290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423127b-5725-436c-b806-615ce6621569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21e162-4c16-423b-83cc-c2b04138dea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6bb4db-8551-4847-898b-bd0394f2a955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c3869-56ce-4647-9bd5-668bb9422a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc683b-7e43-4771-acab-7ad83849c2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78547136-6c19-45c1-a4ad-7d4008c211c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d87af3-a163-4a29-8fc1-78350d706490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6006d-c603-4e48-b271-2ef31d7fc1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b46769-ea8c-45cd-8987-290f04c62c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb82822-07f9-41dd-8f62-a637f1813eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af67fb7-f570-4782-a98f-ea7d60a30acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf977e47-4f3e-40c3-8152-ab2272643061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34889602-8df5-4081-a46c-68bfbbb6d91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90eacc-863d-4ef4-813b-c4ad3a16d37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89b899-cdc7-487b-bc64-2e0e36ca791a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc413c6-9dd6-454a-9c00-e749f758d27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dda9b8-85e7-4f71-84fd-ab57cac616e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0b489-0d1e-46c7-aec7-4a08b793e277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d0e77-b2b8-4459-b28c-1ec83142497f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfcc8e-b47c-4e6e-8dbe-5ef1e88bb5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641659a-ed64-4540-bf45-23bf401235f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b9a8a-e723-409f-8754-079ae3ee7e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0bc5d-26e0-4581-a2ce-826d2d68d2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7ee63-b2f5-4b90-8a8d-3d6a27ba51a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc7831-f9b3-4041-8dcd-945dd57e9555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c9d82-73cf-4598-b418-631cb0bcd3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
