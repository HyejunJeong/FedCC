{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ff8712-121e-4c55-9b57-eb317f36a195",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plan\n",
    "\n",
    "- [x] FL base framework\n",
    "- [x] implement non-iid sampling function following dirichlet dist\n",
    "\n",
    "#### Attack simulation\n",
    "- [x] untargeted model poisoning\n",
    "    - Local model poisoning attacks to Byzantine-robust federated learning <br>\n",
    "    https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning fang attack <br>\n",
    "    krum-attack, trimmed-mean/median attack\n",
    "- [x] targeted model poisoning\n",
    "    - Analyzing federated learning through an adversarial lens <br>\n",
    "    https://github.com/inspire-group/ModelPoisoning\n",
    "- [x] data poisoning\n",
    "    - DBA: Distributed backdoor attacks against federated learning <br>\n",
    "    https://github.com/AI-secure/DBA\n",
    "    \n",
    "#### Defense baseline\n",
    "- [x] FedAvg\n",
    "- [x] Krum\n",
    "- [x] Multi-Krum\n",
    "- [x] Bulyan\n",
    "- [x] Coordinate median\n",
    "- [x] FLARE\n",
    "\n",
    "#### Proposed method\n",
    "- [x] extract PLRs\n",
    "- [x] apply RBF hypersphere CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97834e9-666d-4e4a-a15e-332181ee70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e5affa-9f28-4dad-99af-52f285107603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, mal_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, Alexnet, modelC, LeNet5\n",
    "from utils import get_dataset, get_mal_dataset, exp_details, flatten, construct_ordered_dict\n",
    "from aggregate import fedavg, multi_krum, krum, coomed, bulyan, tr_mean, fed_align, fed_cc, flare, fltrust\n",
    "from attacks import get_malicious_updates_untargeted_mkrum, get_malicious_updates_untargeted_med, get_malicious_updates_targeted\n",
    "from cka import linear_CKA, kernel_CKA\n",
    "# python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=10\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5ee40-5071-437b-bc8e-35135865c450",
   "metadata": {},
   "source": [
    "# Targeted Model Poisoning Attack (label flipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea6ac2c-32ca-417f-90fb-f5b58b7ad1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 20\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 3 # num of local epoch\n",
    "    local_bs = 64 # batch size\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    aggregation = 'fedavg' # fedavg, krum, mkrum, coomed, bulyan, flare, fedcc\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'fmnist' # fmnist, cifar, mnist\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes = 100 \n",
    "        num_channels = 3 # num of channels of imgs\n",
    "    else:\n",
    "        num_classes = 10\n",
    "        num_channels = 1\n",
    "    \n",
    "    gpu = 0\n",
    "    optimizer = 'sgd'\n",
    "    iid = 1 # 0 for non-iid\n",
    "    alpha = 0.2 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_clients = [0,1,2,3] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 5 # number of maliciuos data sample\n",
    "    mal_lr = 0.005\n",
    "    local_mal_ep = 6\n",
    "    boost = 2 # alpha: 2 for fedavg, 3.5 for krum\n",
    "    mal_bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0462f137-23d4-4d56-8de6-1012b0181ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : cnn\n",
      "    Optimizer : sgd\n",
      "    Learning  : 0.01\n",
      "    Aggregation     : fedavg\n",
      "    Global Rounds   : 20\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    Fraction of users    : 1\n",
      "    Local Batch size     : 64\n",
      "    Local Epochs         : 3\n",
      "\n",
      "    Malicious parameters:\n",
      "    Attackers            : [0, 1, 2, 3]\n",
      "    Attack Type          : targeted\n",
      "CNNFashion_Mnist(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=25600, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "malcious dataset true labels: 7, malicious labels: [9, 9, 9, 9, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "| Global Training Round : 1 |\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjeong_umass_edu/.conda/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0, loss 0.6270526064592495, acc 81.66666666666667, mal loss 0.6820063591003418, mal acc 80.0\n",
      "user 1, loss 0.643422057852149, acc 79.66666666666666, mal loss 0.8096345067024231, mal acc 80.0\n",
      "user 2, loss 0.6183002554370385, acc 80.66666666666666, mal loss 0.40721702575683594, mal acc 100.0\n",
      "user 3, loss 0.6302642865727345, acc 83.66666666666667, mal loss 0.7034151554107666, mal acc 100.0\n",
      "user 4, loss 0.6774219682481554, acc 81.83333333333334\n",
      "user 5, loss 0.6925896481672922, acc 80.66666666666666\n",
      "user 6, loss 0.7062370313538445, acc 81.66666666666667\n",
      "user 7, loss 0.6780280296007791, acc 82.16666666666667\n",
      "user 8, loss 0.6837983073128595, acc 81.33333333333333\n",
      "user 9, loss 0.7248590660095214, acc 80.83333333333333\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 0.6681973257013624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:21<06:39, 21.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 80.04% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.81, confidence: tensor([0.0006, 0.0029, 0.0053, 0.0012, 0.0211])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 2 |\n",
      "=========================================\n",
      "user 0, loss 0.3528284011898856, acc 84.66666666666667, mal loss 0.04025585949420929, mal acc 100.0\n",
      "user 1, loss 0.36888545199313705, acc 82.66666666666667, mal loss 0.07552474737167358, mal acc 100.0\n",
      "user 2, loss 0.34766923374774167, acc 82.83333333333334, mal loss 0.029640858992934227, mal acc 100.0\n",
      "user 3, loss 0.3453940854881678, acc 83.66666666666667, mal loss 0.012029449455440044, mal acc 100.0\n",
      "user 4, loss 0.3684672062926822, acc 83.0\n",
      "user 5, loss 0.3797462555434969, acc 86.0\n",
      "user 6, loss 0.39673195540905, acc 86.33333333333333\n",
      "user 7, loss 0.37338574330012003, acc 85.33333333333334\n",
      "user 8, loss 0.36581916146808197, acc 86.16666666666667\n",
      "user 9, loss 0.38454149358802375, acc 86.16666666666667\n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training Loss : 0.5182721122517006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:42<06:19, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 86.79% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 6.20, confidence: tensor([0.0010, 0.0003, 0.0149, 0.0040, 0.0021])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 3 |\n",
      "=========================================\n",
      "user 0, loss 0.2336464220037063, acc 86.66666666666667, mal loss 0.007226706948131323, mal acc 100.0\n",
      "user 1, loss 0.24878283420152833, acc 85.83333333333333, mal loss 0.010681947693228722, mal acc 100.0\n",
      "user 2, loss 0.23426464688555712, acc 85.0, mal loss 0.010963888838887215, mal acc 100.0\n",
      "user 3, loss 0.2284793574362993, acc 87.16666666666667, mal loss 0.019302647560834885, mal acc 100.0\n",
      "user 4, loss 0.29043713953759936, acc 85.5\n",
      "user 5, loss 0.3007071203655667, acc 86.83333333333333\n",
      "user 6, loss 0.30447367860211266, acc 87.16666666666667\n",
      "user 7, loss 0.29675637592871984, acc 87.66666666666667\n",
      "user 8, loss 0.2846468566192521, acc 88.0\n",
      "user 9, loss 0.2958892338805728, acc 86.5\n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training Loss : 0.43611753034983086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:03<06:00, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 88.54% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 9.90, confidence: tensor([1.9078e-05, 1.0316e-04, 6.5724e-06, 6.9887e-06, 3.5611e-03])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 4 |\n",
      "=========================================\n",
      "user 0, loss 0.17286841846234577, acc 86.16666666666667, mal loss 0.016335006803274155, mal acc 100.0\n",
      "user 1, loss 0.1830921686507696, acc 85.0, mal loss 0.019790716469287872, mal acc 100.0\n",
      "user 2, loss 0.17140903260166707, acc 87.0, mal loss 0.004894697107374668, mal acc 100.0\n",
      "user 3, loss 0.1732120960215597, acc 86.16666666666667, mal loss 0.01784670539200306, mal acc 100.0\n",
      "user 4, loss 0.25548429552051755, acc 87.0\n",
      "user 5, loss 0.2645042683349715, acc 86.33333333333333\n",
      "user 6, loss 0.2622986395822631, acc 86.83333333333333\n",
      "user 7, loss 0.2548699487580193, acc 87.0\n",
      "user 8, loss 0.2493265222509702, acc 85.66666666666667\n",
      "user 9, loss 0.252203498284022, acc 86.0\n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training Loss : 0.3830698699740508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:24<05:39, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 88.67% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 9.38, confidence: tensor([4.8167e-05, 4.0346e-05, 4.8928e-04, 6.3168e-04, 7.0892e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 5 |\n",
      "=========================================\n",
      "user 0, loss 0.13862524789414907, acc 86.5, mal loss 0.0121835982427001, mal acc 100.0\n",
      "user 1, loss 0.14767035868492698, acc 87.66666666666667, mal loss 0.019035454839468002, mal acc 100.0\n",
      "user 2, loss 0.14755546876735853, acc 87.0, mal loss 0.0031513452995568514, mal acc 100.0\n",
      "user 3, loss 0.1371762348918996, acc 89.66666666666666, mal loss 0.08473721891641617, mal acc 100.0\n",
      "user 4, loss 0.22455196330944696, acc 87.16666666666667\n",
      "user 5, loss 0.2382065710425377, acc 87.33333333333333\n",
      "user 6, loss 0.24074905069337948, acc 87.83333333333333\n",
      "user 7, loss 0.2286805563999547, acc 88.0\n",
      "user 8, loss 0.22033159022529922, acc 89.0\n",
      "user 9, loss 0.2201579425897863, acc 87.66666666666667\n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training Loss : 0.3453299956692154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:45<05:17, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.29% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 11.48, confidence: tensor([1.1229e-07, 2.6833e-05, 7.0932e-05, 1.1708e-04, 4.7615e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 6 |\n",
      "=========================================\n",
      "user 0, loss 0.1113970436854288, acc 87.66666666666667, mal loss 0.008030777797102928, mal acc 100.0\n",
      "user 1, loss 0.12313907336353798, acc 86.5, mal loss 0.020308425650000572, mal acc 100.0\n",
      "user 2, loss 0.12638689661112598, acc 87.5, mal loss 0.005248176399618387, mal acc 100.0\n",
      "user 3, loss 0.11172253417725299, acc 88.5, mal loss 0.00928876455873251, mal acc 100.0\n",
      "user 4, loss 0.20540408555004333, acc 87.0\n",
      "user 5, loss 0.21736460412542025, acc 86.66666666666667\n",
      "user 6, loss 0.22122865796089175, acc 87.16666666666667\n",
      "user 7, loss 0.2102776779068841, acc 89.33333333333333\n",
      "user 8, loss 0.2013616005248494, acc 88.33333333333333\n",
      "user 9, loss 0.20516560895575417, acc 86.83333333333333\n",
      " \n",
      "Avg Training Stats after 6 global rounds:\n",
      "Training Loss : 0.31666579277203266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [02:07<04:56, 21.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.12% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 11.52, confidence: tensor([5.6460e-08, 1.1615e-05, 2.0000e-05, 3.4942e-04, 2.0898e-05])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 7 |\n",
      "=========================================\n",
      "user 0, loss 0.0960557154122446, acc 87.83333333333333, mal loss 0.0029334735590964556, mal acc 100.0\n",
      "user 1, loss 0.10790389824225649, acc 85.66666666666667, mal loss 0.015023112297058105, mal acc 100.0\n",
      "user 2, loss 0.10874873578237991, acc 88.0, mal loss 0.0012791731860488653, mal acc 100.0\n",
      "user 3, loss 0.09363412261907861, acc 88.33333333333333, mal loss 0.0019469615072011948, mal acc 100.0\n",
      "user 4, loss 0.1951954478936063, acc 87.16666666666667\n",
      "user 5, loss 0.210994601017899, acc 88.66666666666667\n",
      "user 6, loss 0.20380866703059938, acc 87.83333333333333\n",
      "user 7, loss 0.20218426292141278, acc 87.16666666666667\n",
      "user 8, loss 0.1904540997909175, acc 89.5\n",
      "user 9, loss 0.19196047397951285, acc 88.33333333333333\n",
      " \n",
      "Avg Training Stats after 7 global rounds:\n",
      "Training Loss : 0.2942983941573124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [02:28<04:36, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.56% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 13.71, confidence: tensor([5.4203e-06, 2.5538e-05, 1.1070e-10, 7.6773e-06, 1.4669e-05])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 8 |\n",
      "=========================================\n",
      "user 0, loss 0.0813888918073325, acc 87.5, mal loss 0.008009561337530613, mal acc 100.0\n",
      "user 1, loss 0.09049366403926622, acc 86.33333333333333, mal loss 0.001315420726314187, mal acc 100.0\n",
      "user 2, loss 0.09509760309051592, acc 88.66666666666667, mal loss 0.008629529736936092, mal acc 100.0\n",
      "user 3, loss 0.08112498822932442, acc 90.83333333333333, mal loss 0.004227896220982075, mal acc 100.0\n",
      "user 4, loss 0.1782347849259774, acc 87.5\n",
      "user 5, loss 0.19399462746249305, acc 87.83333333333333\n",
      "user 6, loss 0.19206286816133392, acc 88.16666666666667\n",
      "user 7, loss 0.188140260775884, acc 87.16666666666667\n",
      "user 8, loss 0.1729061340706216, acc 88.16666666666667\n",
      "user 9, loss 0.17874258736769358, acc 88.33333333333333\n",
      " \n",
      "Avg Training Stats after 8 global rounds:\n",
      "Training Loss : 0.2756634250117789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:49<04:13, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.85% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 12.33, confidence: tensor([1.9819e-06, 4.5321e-08, 3.9872e-05, 4.9619e-06, 9.3787e-05])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 9 |\n",
      "=========================================\n",
      "user 0, loss 0.0736085375681062, acc 88.66666666666667, mal loss 0.05901901796460152, mal acc 100.0\n",
      "user 1, loss 0.07741780068190246, acc 87.0, mal loss 0.003601374104619026, mal acc 100.0\n",
      "user 2, loss 0.08679260643762782, acc 89.0, mal loss 0.002858149353414774, mal acc 100.0\n",
      "user 3, loss 0.07142403479634883, acc 90.33333333333333, mal loss 0.07243362814188004, mal acc 100.0\n",
      "user 4, loss 0.17228095601830218, acc 86.33333333333333\n",
      "user 5, loss 0.18812519384755025, acc 88.0\n",
      "user 6, loss 0.17615216991139782, acc 88.33333333333333\n",
      "user 7, loss 0.17609657959805594, acc 86.5\n",
      "user 8, loss 0.1703572317875094, acc 88.83333333333333\n",
      "user 9, loss 0.1626924377017551, acc 86.16666666666667\n",
      " \n",
      "Avg Training Stats after 9 global rounds:\n",
      "Training Loss : 0.26008912832545406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [03:10<03:53, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.55% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 13.76, confidence: tensor([4.2110e-07, 1.0481e-06, 1.1518e-06, 4.8545e-06, 5.3378e-07])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 10 |\n",
      "=========================================\n",
      "user 0, loss 0.06314282366146105, acc 89.16666666666667, mal loss 0.004230614751577377, mal acc 100.0\n",
      "user 1, loss 0.06961070107737262, acc 88.16666666666667, mal loss 0.013107463717460632, mal acc 100.0\n",
      "user 2, loss 0.08615305930531274, acc 87.33333333333333, mal loss 0.0009373066131956875, mal acc 100.0\n",
      "user 3, loss 0.06454052010922808, acc 90.33333333333333, mal loss 0.04472721368074417, mal acc 100.0\n",
      "user 4, loss 0.166881033844418, acc 88.33333333333333\n",
      "user 5, loss 0.17577302848299345, acc 87.66666666666667\n",
      "user 6, loss 0.16751062117516993, acc 87.5\n",
      "user 7, loss 0.1718650705450111, acc 86.83333333333333\n",
      "user 8, loss 0.1625262234856685, acc 89.33333333333333\n",
      "user 9, loss 0.1512935843070348, acc 89.33333333333333\n",
      " \n",
      "Avg Training Stats after 10 global rounds:\n",
      "Training Loss : 0.24687318215284537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [03:32<03:32, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.57% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 13.43, confidence: tensor([1.0259e-05, 1.2595e-08, 1.9142e-05, 1.0297e-08, 2.7139e-04])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 11 |\n",
      "=========================================\n",
      "user 0, loss 0.060638743745297874, acc 87.66666666666667, mal loss 0.0027693291194736958, mal acc 100.0\n",
      "user 1, loss 0.061884440730321795, acc 86.83333333333333, mal loss 0.03929262235760689, mal acc 100.0\n",
      "user 2, loss 0.07335236200474594, acc 88.16666666666667, mal loss 0.018928099423646927, mal acc 100.0\n",
      "user 3, loss 0.05924797926497392, acc 91.33333333333333, mal loss 0.0036435741931200027, mal acc 100.0\n",
      "user 4, loss 0.15637496040099197, acc 86.83333333333333\n",
      "user 5, loss 0.17366860521336394, acc 87.66666666666667\n",
      "user 6, loss 0.1630550971875588, acc 87.5\n",
      "user 7, loss 0.1626984598984321, acc 87.66666666666667\n",
      "user 8, loss 0.14980550573103957, acc 89.33333333333333\n",
      "user 9, loss 0.14904716447823577, acc 88.66666666666667\n",
      " \n",
      "Avg Training Stats after 11 global rounds:\n",
      "Training Loss : 0.23542810485399546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:53<03:11, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.81% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 15.81, confidence: tensor([1.0479e-06, 3.7925e-09, 3.2148e-09, 4.5022e-07, 8.2831e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 12 |\n",
      "=========================================\n",
      "user 0, loss 0.05769658367314964, acc 88.66666666666667, mal loss 0.003780986415222287, mal acc 100.0\n",
      "user 1, loss 0.05693424586410913, acc 87.5, mal loss 0.00364127429202199, mal acc 100.0\n",
      "user 2, loss 0.06838306148175002, acc 89.16666666666667, mal loss 0.0009597347234375775, mal acc 100.0\n",
      "user 3, loss 0.05074487176809048, acc 90.0, mal loss 0.002219616435468197, mal acc 100.0\n",
      "user 4, loss 0.1465825576417976, acc 86.66666666666667\n",
      "user 5, loss 0.16375397875905037, acc 88.66666666666667\n",
      "user 6, loss 0.15370913083354631, acc 87.83333333333333\n",
      "user 7, loss 0.16720129425326982, acc 86.83333333333333\n",
      "user 8, loss 0.15385301839146348, acc 89.33333333333333\n",
      "user 9, loss 0.14492020805676778, acc 87.33333333333333\n",
      " \n",
      "Avg Training Stats after 12 global rounds:\n",
      "Training Loss : 0.2255072540388541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [04:14<02:50, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.86% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 14.38, confidence: tensor([3.8921e-09, 2.4352e-06, 1.5901e-05, 5.2755e-06, 7.6197e-08])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 13 |\n",
      "=========================================\n",
      "user 0, loss 0.0449440864785605, acc 87.5, mal loss 0.0011041872203350067, mal acc 100.0\n",
      "user 1, loss 0.04883765973525506, acc 87.83333333333333, mal loss 0.30883002281188965, mal acc 80.0\n",
      "user 2, loss 0.07163300884738481, acc 88.16666666666667, mal loss 0.0007435858133248985, mal acc 100.0\n",
      "user 3, loss 0.04588167192649276, acc 91.0, mal loss 0.0025059471372514963, mal acc 100.0\n",
      "user 4, loss 0.14781662635091278, acc 88.66666666666667\n",
      "user 5, loss 0.15512535375025535, acc 87.66666666666667\n",
      "user 6, loss 0.1458148203790188, acc 88.5\n",
      "user 7, loss 0.14938438205255403, acc 86.16666666666667\n",
      "user 8, loss 0.14098697510858377, acc 90.83333333333333\n",
      "user 9, loss 0.1412034220372637, acc 87.5\n",
      " \n",
      "Avg Training Stats after 13 global rounds:\n",
      "Training Loss : 0.21655768070252904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [04:35<02:29, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.84% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 19.37, confidence: tensor([1.5100e-11, 1.4838e-10, 2.8963e-07, 7.6834e-10, 1.7280e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 14 |\n",
      "=========================================\n",
      "user 0, loss 0.04701484583845166, acc 88.33333333333333, mal loss 0.012619413435459137, mal acc 100.0\n",
      "user 1, loss 0.04670890588456308, acc 89.5, mal loss 0.00040432121022604406, mal acc 100.0\n",
      "user 2, loss 0.05862128419552423, acc 90.0, mal loss 0.00043785269372165203, mal acc 100.0\n",
      "user 3, loss 0.04672602278981031, acc 90.33333333333333, mal loss 0.002936630044132471, mal acc 100.0\n",
      "user 4, loss 0.1375232536594073, acc 87.33333333333333\n",
      "user 5, loss 0.15139281283236214, acc 87.0\n",
      "user 6, loss 0.14393235757119124, acc 88.66666666666667\n",
      "user 7, loss 0.14619628476599852, acc 88.5\n",
      "user 8, loss 0.13620020161486335, acc 89.83333333333333\n",
      "user 9, loss 0.13317749961796735, acc 89.5\n",
      " \n",
      "Avg Training Stats after 14 global rounds:\n",
      "Training Loss : 0.20857137114356367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [04:56<02:07, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 90.00% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 14.74, confidence: tensor([6.2068e-07, 8.0961e-07, 7.5635e-10, 1.8122e-05, 1.4520e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 15 |\n",
      "=========================================\n",
      "user 0, loss 0.03968346370432604, acc 90.5, mal loss 0.0004780317540280521, mal acc 100.0\n",
      "user 1, loss 0.04820254937620025, acc 86.83333333333333, mal loss 0.0023332596756517887, mal acc 100.0\n",
      "user 2, loss 0.06690346675653322, acc 88.83333333333333, mal loss 0.000443519267719239, mal acc 100.0\n",
      "user 3, loss 0.042222221658703984, acc 90.66666666666666, mal loss 0.025304075330495834, mal acc 100.0\n",
      "user 4, loss 0.13173270367085935, acc 89.5\n",
      "user 5, loss 0.15295788573722044, acc 88.16666666666667\n",
      "user 6, loss 0.1341446291903655, acc 88.33333333333333\n",
      "user 7, loss 0.135704322407643, acc 85.83333333333333\n",
      "user 8, loss 0.12560026287204687, acc 89.33333333333333\n",
      "user 9, loss 0.1292909413948655, acc 87.33333333333333\n",
      " \n",
      "Avg Training Stats after 15 global rounds:\n",
      "Training Loss : 0.20137622937911784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [05:18<01:45, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.98% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 16.19, confidence: tensor([2.7767e-07, 4.6153e-08, 3.4401e-08, 2.2357e-07, 6.9344e-08])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 16 |\n",
      "=========================================\n",
      "user 0, loss 0.039987586568857196, acc 89.66666666666666, mal loss 0.0011090198531746864, mal acc 100.0\n",
      "user 1, loss 0.0408547054580515, acc 87.83333333333333, mal loss 0.04274861887097359, mal acc 100.0\n",
      "user 2, loss 0.05214722256080182, acc 88.83333333333333, mal loss 0.00202577724121511, mal acc 100.0\n",
      "user 3, loss 0.03969696605820087, acc 88.66666666666667, mal loss 0.0028927114326506853, mal acc 100.0\n",
      "user 4, loss 0.13048127989388175, acc 87.83333333333333\n",
      "user 5, loss 0.14078002295560307, acc 88.33333333333333\n",
      "user 6, loss 0.134173122478856, acc 86.16666666666667\n",
      "user 7, loss 0.13392195676763852, acc 86.5\n",
      "user 8, loss 0.12546609434816572, acc 90.0\n",
      "user 9, loss 0.11629933343993293, acc 88.5\n",
      " \n",
      "Avg Training Stats after 16 global rounds:\n",
      "Training Loss : 0.19475151685873543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [05:37<01:23, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 89.87% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 16.86, confidence: tensor([6.8155e-11, 4.4548e-06, 6.3372e-06, 1.7191e-08, 7.3383e-09])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 17 |\n",
      "=========================================\n",
      "user 0, loss 0.03731223973731527, acc 88.16666666666667, mal loss 0.0034196064807474613, mal acc 100.0\n",
      "user 1, loss 0.042546114608596426, acc 89.33333333333333, mal loss 0.009967758320271969, mal acc 100.0\n",
      "user 2, loss 0.04982655242150812, acc 89.66666666666666, mal loss 0.0007492214208468795, mal acc 100.0\n",
      "user 3, loss 0.039728551104832276, acc 89.66666666666666, mal loss 0.000991156091913581, mal acc 100.0\n",
      "user 4, loss 0.12427066815810071, acc 88.5\n",
      "user 5, loss 0.12844700425863265, acc 88.83333333333333\n",
      "user 6, loss 0.12839322417974472, acc 89.16666666666667\n",
      "user 7, loss 0.13083466296394666, acc 86.0\n",
      "user 8, loss 0.12330219696379369, acc 88.83333333333333\n",
      "user 9, loss 0.11921894476231602, acc 88.5\n",
      " \n",
      "Avg Training Stats after 17 global rounds:\n",
      "Training Loss : 0.18873013445033207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [05:57<01:01, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 90.00% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 16.40, confidence: tensor([1.1795e-09, 1.1979e-09, 1.2134e-05, 7.3870e-06, 1.8942e-08])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 18 |\n",
      "=========================================\n",
      "user 0, loss 0.03519260104803834, acc 88.66666666666667, mal loss 0.009904688224196434, mal acc 100.0\n",
      "user 1, loss 0.04112520149506612, acc 89.16666666666667, mal loss 0.0019205143908038735, mal acc 100.0\n",
      "user 2, loss 0.04822526793935123, acc 88.5, mal loss 0.0004185800498817116, mal acc 100.0\n",
      "user 3, loss 0.03827737212097354, acc 91.16666666666666, mal loss 0.019101470708847046, mal acc 100.0\n",
      "user 4, loss 0.12230771168859471, acc 89.0\n",
      "user 5, loss 0.12165502935441004, acc 88.66666666666667\n",
      "user 6, loss 0.12597340424855552, acc 86.66666666666667\n",
      "user 7, loss 0.1263796037932237, acc 89.5\n",
      "user 8, loss 0.12262354315982922, acc 90.5\n",
      "user 9, loss 0.10784866960512268, acc 86.66666666666667\n",
      " \n",
      "Avg Training Stats after 18 global rounds:\n",
      "Training Loss : 0.1831873958944979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [06:17<00:40, 20.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 90.38% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 15.95, confidence: tensor([6.9127e-07, 1.7992e-09, 2.2124e-06, 4.6243e-09, 1.8568e-06])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 19 |\n",
      "=========================================\n",
      "user 0, loss 0.033095167646247876, acc 88.83333333333333, mal loss 0.0015710864681750536, mal acc 100.0\n",
      "user 1, loss 0.035754300756308484, acc 88.0, mal loss 0.0032966521102935076, mal acc 100.0\n",
      "user 2, loss 0.05048936638080417, acc 89.5, mal loss 5.8313878980698064e-05, mal acc 100.0\n",
      "user 3, loss 0.031512865103171576, acc 90.5, mal loss 0.0007316942792385817, mal acc 100.0\n",
      "user 4, loss 0.12089519228786229, acc 87.0\n",
      "user 5, loss 0.13000481763647662, acc 88.5\n",
      "user 6, loss 0.11762077649434406, acc 88.33333333333333\n",
      "user 7, loss 0.12591658976756864, acc 87.83333333333333\n",
      "user 8, loss 0.11733076020661327, acc 89.5\n",
      "user 9, loss 0.11139797281059954, acc 86.5\n",
      " \n",
      "Avg Training Stats after 19 global rounds:\n",
      "Training Loss : 0.1781460477373664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [06:38<00:20, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 90.03% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 17.72, confidence: tensor([4.2582e-08, 6.6317e-07, 4.5926e-07, 5.7780e-10, 4.4101e-10])\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 20 |\n",
      "=========================================\n",
      "user 0, loss 0.028985211298149488, acc 89.66666666666666, mal loss 0.03367520496249199, mal acc 100.0\n",
      "user 1, loss 0.034318327132240756, acc 86.83333333333333, mal loss 0.009656304493546486, mal acc 100.0\n",
      "user 2, loss 0.050701007364790586, acc 89.33333333333333, mal loss 0.0025065012741833925, mal acc 100.0\n",
      "user 3, loss 0.034151665624864234, acc 90.33333333333333, mal loss 0.002521452261134982, mal acc 100.0\n",
      "user 4, loss 0.11751363807254367, acc 89.83333333333333\n",
      "user 5, loss 0.127772315127982, acc 87.66666666666667\n",
      "user 6, loss 0.11769658866441912, acc 87.66666666666667\n",
      "user 7, loss 0.11882446966237492, acc 87.5\n",
      "user 8, loss 0.11558855116056899, acc 89.66666666666666\n",
      "user 9, loss 0.11352843753993512, acc 87.33333333333333\n",
      " \n",
      "Avg Training Stats after 20 global rounds:\n",
      "Training Loss : 0.1735341464087374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:59<00:00, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 90.10% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 19.00, confidence: tensor([1.0404e-07, 6.1119e-10, 8.9434e-09, 4.9159e-11, 1.9546e-07])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 20 global rounds of training:\n",
      "|---- Test Benign Accuracy: 90.02%\n",
      "|---- Test Malicious Accuracy: 0.00%, Malicious Loss: 19.26, confidence:4.699540667729707e-08\n",
      "\n",
      "\n",
      " Total Run Time: 420.4799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if args.gpu == 0 else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "        elif args.dataset == 'cifar100':\n",
    "            global_model = LeNet5(args=args)\n",
    "\n",
    "            \n",
    "    # elif args.model == 'alexnet':\n",
    "    #     if args.dataset == 'cifar100':\n",
    "    #         global_model = Alexnet(args=args)\n",
    "    \n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "        print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    \n",
    "    confidence = []    \n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "            \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_clients:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            else:\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            # boost the malicious (weight+bias) by alpha\n",
    "            # if mal_user:\n",
    "            #     flat_delta_m = flatten(w) - flatten(w_prev)\n",
    "            #     flat_mal_w = flatten(w_prev) + args.boost * flat_delta_m\n",
    "            #     w = construct_ordered_dict(global_model, torch.tensor(flat_mal_w).to(device))\n",
    "              \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "            \n",
    "            # if agg==krum, alternate training\n",
    "            if args.aggregation == 'krum':\n",
    "                if idx in args.mal_clients and epoch %2 ==0:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else:                     \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "            else:\n",
    "                # if malicious user: boost the detlta weights\n",
    "                if idx in args.mal_clients:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else: \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_clients)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif args.aggregation == 'fltrust':\n",
    "            glob_weights = []\n",
    "            glob_weights.append(flatten(global_weights))\n",
    "            agg_weights = fltrust(malicious_grads, glob_weights)\n",
    "\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]            \n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, selected_idxs = fed_cc(malicious_grads, glob_plr, plrs, 'kernel')\n",
    "            print(f'fed_cc Selected idx: {selected_idxs}')\n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "            \n",
    "            if len(args.mal_clients) > 0:\n",
    "                mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "                print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "                confidence.append(mal_out[0].item())\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    \n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "        print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out[0]))\n",
    "    \n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda3a20-2321-4637-912f-1cf8e2b6dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_weights = []\n",
    "for key in global_weights:\n",
    "    if 'weight' in key:\n",
    "        only_weights = np.append(only_weights, global_weights[key].detach().cpu().numpy())\n",
    "_, glob_svd, _, = np.linalg.svd(only_weights.reshape(1,-1))\n",
    "glob_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b17f74-9b4f-40eb-886f-1000bf52b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('euclidean')\n",
    "for i in range(len(plrs)):\n",
    "    val = euclidean_distances(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    # print(val)\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63086320-b1f9-46ff-aed0-34819c44ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, paired_cosine_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('cosine')\n",
    "for i in range(len(plrs)):\n",
    "    val = cosine_similarity(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d998913-cbd5-4897-92f2-3d5bb6661a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5079-8318-48dc-9c60-de72aeef88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv1_weights = np.array(conv1_weights)\n",
    "conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv2_weights = np.array(conv2_weights)\n",
    "conv3_weights = [np.array(w['conv3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv3_weights = np.array(conv3_weights)\n",
    "fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc1_weights = np.array(fc1_weights)\n",
    "fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc2_weights = np.array(fc2_weights)\n",
    "\n",
    "num_layers = 5\n",
    "\n",
    "_, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "_, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "_, s_layer_2, _, = np.linalg.svd(conv3_weights)\n",
    "_, s_layer_3, _, = np.linalg.svd(fc1_weights)\n",
    "_, s_layer_4, _, = np.linalg.svd(fc2_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae72f18-e639-4408-a741-95151b72caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d6ead-4de5-4907-b640-037f51e08da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in zip(np.arange(num_layers), [s_layer_0, s_layer_1, s_layer_2, s_layer_3, s_layer_4]):\n",
    "    plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "    for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "        plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "plt.xticks(np.arange(num_layers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd88e6-ccbc-463e-8502-8364fd57a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_data = linkage(conv1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer1')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e212a-0411-4bd5-a0c8-a8811f3b09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer2')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb776-55aa-47e1-a710-1b3801e338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv3_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer3')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d785aca-b39b-45d9-a812-406235b4ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: PLR')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3a20f-0f1a-45d8-8942-aa1916f9f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer5')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c78f9d-cc6c-459d-8758-2404a5d83454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e286-f726-4044-95fb-ec25b21ab4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead0e8a-557b-4b3e-8d97-af9370d31ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8437cfc-68ed-48fc-8d82-9fc9bdfb4288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f315-00f2-4d04-87de-8acc99e07788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66663c2d-4265-4388-ac5c-715ee748a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "# conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv1_weights = np.array(conv1_weights)\n",
    "# conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv2_weights = np.array(conv2_weights)\n",
    "# fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc1_weights = np.array(fc1_weights)\n",
    "# fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc2_weights = np.array(fc2_weights)\n",
    "# fc3_weights = [np.array(w['fc3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc3_weights = np.array(fc3_weights)\n",
    "\n",
    "# num_layers = 5\n",
    "\n",
    "# _, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "# _, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "# _, s_layer_2, _, = np.linalg.svd(fc1_weights)\n",
    "# _, s_layer_3, _, = np.linalg.svd(fc2_weights)\n",
    "# _, s_layer_4, _, = np.linalg.svd(fc3_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3e820-c2be-48cc-b2da-f05321c4162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for x, y in zip(np.arange(num_layers-1), [s_layer_0, s_layer_1, s_layer_3, s_layer_4]):\n",
    "#     plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "#     for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "#         plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "# plt.xticks(np.arange(num_layers))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506c011-6105-4c3e-8c2c-d49c25ba4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(args.mal_clients) > 0:\n",
    "#     import csv\n",
    "#     if args.iid:\n",
    "#         filename = 'Confidence_2IID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "#     else:\n",
    "#         filename = 'Confidence_2NIID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "\n",
    "#     with open(filename, mode='w', newline='') as csvfile:\n",
    "#         w = csv.writer(csvfile)\n",
    "#         w.writerows(map(lambda x: [x], confidence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdac6fe-33d4-483e-97a8-3c6492435ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mal_dataset_of_class\n",
    "\n",
    "test_mal_X_list, test_mal_Y, test_Y_true = get_mal_dataset_of_class(test_dataset, args.num_mal*100, Y_true, mal_Y)\n",
    "flat_test_mal_Y = np.hstack([y for y in test_mal_Y])\n",
    "mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, test_mal_X_list, flat_test_mal_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd663b-a50f-4009-b8bb-6d649683eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_acc, mal_loss, torch.mean(mal_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26323f6a-6e52-4e2e-901c-ffeb1c2d2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset, args.boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399b8e9-1931-4681-a07a-8e9e5a692ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_details(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4c375-638b-44ec-99b1-ee0de7e75a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # PLOTTING (optional)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.figure()\n",
    "plt.title('Training Loss vs Communication rounds')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))\n",
    "\n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
