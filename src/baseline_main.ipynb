{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0462f137-23d4-4d56-8de6-1012b0181ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import get_dataset\n",
    "# from options import args_parser\n",
    "from update import test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "\n",
    "# python src/baseline_main.py --model=mlp --dataset=mnist --epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19464d6-6f8b-4d6c-bcb9-041ec14ec25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 10\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 5 # num of local epoch\n",
    "    local_bs = 128 # batch size\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    num_channels = 1 # num of channels of imgs\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = \"fmnist\"\n",
    "    num_classes = 10 \n",
    "    gpu = 0\n",
    "    optimizer = 'sgd'\n",
    "    iid = 1 # 0 for non-iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 1\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    n_attackers = [0]\n",
    "    attack_type = None #'untargeted_mkrum' # untargeted_med, untargeted_mkrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ef2c76-7b03-4cde-a925-966cb5b7b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsz : batch size (number of positive pairs)\n",
    "# d   : latent dim\n",
    "# x   : Tensor, shape=[bsz, d]\n",
    "#       latents for one side of positive pairs\n",
    "# y   : Tensor, shape=[bsz, d]\n",
    "#       latents for the other side of positive pairs\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54fdc7b-a0b0-4538-8a19-7b2a4769d484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNFashion_Mnist(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=25600, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299788\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.162196\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.791932\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.277201\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.177069\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.787301\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.867628\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.842936\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.853485\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.734082\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.902456\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.918625\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.727262\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.700110\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.623699\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.736443\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.785856\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.954228\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.704179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:15<02:17, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.003701526663705\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.609887\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.671147\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.582256\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.684173\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.589314\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.671896\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.653801\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.599076\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.751626\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.622268\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.440624\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.739971\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.582485\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.744870\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.728940\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.659028\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.569339\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.620684\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.757026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:27<01:50, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.6446382510128306\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.537952\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.446707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:29<01:56, 14.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[0;32m     47\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[0;32m     50\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cuda113\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    args = Args()\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset, test_dataset, _ = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                               dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # Training\n",
    "    # Set optimizer and criterion\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(global_model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.5)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(global_model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=1e-4)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    epoch_loss = []\n",
    "\n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        batch_loss = []\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = global_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1, batch_idx * len(images), len(trainloader.dataset),\n",
    "                    100. * batch_idx / len(trainloader), loss.item()))\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        loss_avg = sum(batch_loss)/len(batch_loss)\n",
    "        print('\\nTrain loss:', loss_avg)\n",
    "        epoch_loss.append(loss_avg)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Train loss')\n",
    "    plt.savefig('../save/nn_{}_{}_{}.png'.format(args.dataset, args.model,\n",
    "                                                 args.epochs))\n",
    "\n",
    "    # testing\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "    print('Test on', len(test_dataset), 'samples')\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814a5d7-5e42-4318-bf28-076b85e8a991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda113",
   "language": "python",
   "name": "cuda113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
