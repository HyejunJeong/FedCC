{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e82547-9ba7-445e-b7fc-9dedf65f98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from cka import linear_CKA, kernel_CKA, align_loss, uniform_loss, mmd_rbf\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0d7912-47a3-4441-812e-cb8d61a5d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "num_clients = 10\n",
    "num_malicious_clients = 4\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "backdoor_samples_per_batch = 5\n",
    "backdoor_label = 8\n",
    "trigger_value = 255.0  # Backdoor pixel intensity\n",
    "alpha = 1 # 1 for iid\n",
    "scale_factor = 2\n",
    "\n",
    "dataset = \"cifar100\" \n",
    "aggregation_rule = \"fedavg\"  # Choose \"krum\", \"bulyan\", or \"trimmed_mean\", flare, coomed, multi-krum\n",
    "trim_ratio = 0.2  # For Trimmed Mean\n",
    "switch_epoch = epochs // 5 * 3  # Epoch to switch from boosting to robust aggregation\n",
    "method = \"kernel\" # for fedcc\n",
    "\n",
    "device = 'cuda:0'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "class CNNFashion_Mnist(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNNFashion_Mnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "    \n",
    "        self.fc1 = nn.Linear(25600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.dropout(x, .25)\n",
    "        x = x.view(-1, 64 * 20 * 20)\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, .5)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "                      \n",
    "        \n",
    "class CNNCifar(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "    \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 2 * 2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        self.fc1   = nn.Linear(44944, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ab2ebd-ba71-4225-93d4-72c5d00d5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Backdoor Injection\n",
    "def add_backdoor_pattern(images, size, gap, loc, pos=None, trigger_value=1.0):\n",
    "    # Define the top-left corners of the 2x2 grid\n",
    "    if pos == 0:\n",
    "        positions = [(0,0)]\n",
    "    elif pos == 1:\n",
    "        positions = [(0,size+gap)]\n",
    "    elif pos == 2:\n",
    "        positions = [(gap,0)]\n",
    "    elif pos == 3:\n",
    "        positions = [(gap,size+gap)]\n",
    "    else: # centralized\n",
    "        positions = [(0, 0), (0, size+gap), (gap, 0), (gap, size+gap)]  # (row, col) for each box\n",
    "    # Add each 6x1 box to the corresponding position\n",
    "    for row, col in positions:\n",
    "        images[:, row:row + 1, col:col + size] = trigger_value  # Add 6x1 box\n",
    "    return images\n",
    "\n",
    "def inject_backdoor_samples(x, y, num_samples, backdoor_label, position):\n",
    "    indices = np.random.choice(len(x), num_samples, replace=False)\n",
    "    x_backdoor = add_backdoor_pattern(x[indices].clone(), 6, 3, 0, position)\n",
    "    y_backdoor = torch.full((num_samples,), backdoor_label, dtype=torch.long)\n",
    "    x = torch.cat([x, x_backdoor], dim=0)\n",
    "    y = torch.cat([y, y_backdoor], dim=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Create non-IID data distribution using Dirichlet distribution\n",
    "def create_dirichlet_noniid_data(dataset, num_clients, alpha):\n",
    "    num_classes = 10\n",
    "    data_indices = defaultdict(list)\n",
    "\n",
    "    # Group data by class\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        data_indices[label].append(idx)\n",
    "\n",
    "    # Dirichlet distribution to split data among clients\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        class_indices = data_indices[c]\n",
    "        np.random.shuffle(class_indices)\n",
    "        proportions = np.random.dirichlet([alpha] * num_clients)\n",
    "        proportions = (proportions * len(class_indices)).astype(int)\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_clients):\n",
    "            client_indices[i].extend(class_indices[start:start + proportions[i]])\n",
    "            start += proportions[i]\n",
    "\n",
    "    client_datasets = [Subset(dataset, indices) for indices in client_indices]\n",
    "    return client_datasets\n",
    "\n",
    "\n",
    "# Federated Learning Functions\n",
    "def train_local_model(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confidence_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            softmax_outputs = F.softmax(outputs.detach(), dim=1)\n",
    "            confidence_sum += softmax_outputs[range(len(y_batch)), y_batch].sum().item()\n",
    "            \n",
    "    return correct / total, confidence_sum/total\n",
    "\n",
    "\n",
    "# Helper Function to Reconstruct Weights\n",
    "def reconstruct_weights(flat_weights, reference_weights):\n",
    "    aggregated_weights = {}\n",
    "    start_idx = 0\n",
    "\n",
    "    for key, param in reference_weights.items():\n",
    "        num_params = param.numel()\n",
    "        aggregated_weights[key] = flat_weights[start_idx:start_idx + num_params].view(param.size()).to(device)\n",
    "        start_idx += num_params\n",
    "\n",
    "    return aggregated_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076f0c2d-4216-40f4-ac63-78149a81f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean and backdoor test datasets\n",
    "def create_local_test_datasets(dataset, backdoor_label, trigger_value):\n",
    "    clean_test = []\n",
    "    backdoor_test = []\n",
    "\n",
    "    for img, label in dataset:\n",
    "        clean_test.append((img.clone(), label))  # Clean data\n",
    "        img_backdoor = img.clone()\n",
    "        img_backdoor = add_backdoor_pattern(img_backdoor.unsqueeze(0), 6,3,0,trigger_value).squeeze(0)\n",
    "        backdoor_test.append((img_backdoor, backdoor_label))  # Backdoor data\n",
    "\n",
    "    clean_test_loader = DataLoader(clean_test, batch_size=batch_size, shuffle=False)\n",
    "    backdoor_test_loader = DataLoader(backdoor_test, batch_size=batch_size, shuffle=False)\n",
    "    return clean_test_loader, backdoor_test_loader\n",
    "\n",
    "# Create clean and backdoor test datasets\n",
    "def create_test_datasets(dataset, backdoor_label, trigger_value):\n",
    "    clean_test = []\n",
    "    backdoor_test = []\n",
    "    \n",
    "    for img, label in dataset:\n",
    "        clean_test.append((img.clone(), label))  # Clean data\n",
    "        img_backdoor = img.clone()\n",
    "        img_backdoor = add_backdoor_pattern(img_backdoor.unsqueeze(0), 6, 3, 0, trigger_value).squeeze(0)\n",
    "        backdoor_test.append((img_backdoor, backdoor_label))  # Backdoor data\n",
    "\n",
    "    clean_test_loader = DataLoader(clean_test, batch_size=batch_size, shuffle=False)\n",
    "    backdoor_test_loader = DataLoader(backdoor_test, batch_size=batch_size, shuffle=False)\n",
    "    return clean_test_loader, backdoor_test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554b4923-ff53-4450-a704-7389bcbc0e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from cka import linear_CKA, kernel_CKA, align_loss, uniform_loss, mmd_rbf\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Krum aggregation function\n",
    "def krum_aggregation(local_weights, num_clients, num_malicious_clients):\n",
    "    num_neighbors = num_clients - num_malicious_clients - 2  # Number of neighbors to consider\n",
    "    scores = []  # List to store Krum scores\n",
    "\n",
    "    # Flatten weights for distance computation\n",
    "    flat_weights = [torch.cat([param.view(-1) for param in local_weight.values()]) for local_weight in local_weights]\n",
    "\n",
    "    for i, weight_i in enumerate(flat_weights):\n",
    "        # Compute pairwise distances\n",
    "        distances = []\n",
    "        for j, weight_j in enumerate(flat_weights):\n",
    "            if i != j:\n",
    "                distance = torch.norm(weight_i - weight_j, p=2).item()  # Euclidean distance\n",
    "                distances.append((distance, j))\n",
    "        \n",
    "        # Sort distances and sum up the closest `num_neighbors`\n",
    "        distances.sort()\n",
    "        score = sum(d[0] for d in distances[:num_neighbors])\n",
    "        scores.append((score, i))\n",
    "\n",
    "    # Select the client with the minimum Krum score\n",
    "    _, best_client_idx = min(scores)\n",
    "    return local_weights[best_client_idx]\n",
    "\n",
    "# Multi-Krum Aggregation Function\n",
    "def multi_krum_aggregation(local_weights, num_clients, num_malicious_clients, num_to_select):\n",
    "    num_neighbors = num_clients - num_malicious_clients - 2  # Number of neighbors to consider\n",
    "    scores = []  # List to store scores for Multi-Krum\n",
    "\n",
    "    # Flatten weights for distance computation\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "\n",
    "    for i, weight_i in enumerate(flat_weights):\n",
    "        # Compute pairwise distances\n",
    "        distances = []\n",
    "        for j, weight_j in enumerate(flat_weights):\n",
    "            if i != j:\n",
    "                distance = torch.norm(weight_i - weight_j, p=2).item()  # Euclidean distance\n",
    "                distances.append((distance, j))\n",
    "        \n",
    "        # Sort distances and sum up the closest `num_neighbors`\n",
    "        distances.sort()\n",
    "        score = sum(d[0] for d in distances[:num_neighbors])\n",
    "        scores.append((score, i))\n",
    "\n",
    "    # Select the top `num_to_select` clients based on scores\n",
    "    scores.sort()\n",
    "    selected_indices = [idx for _, idx in scores[:num_to_select]]\n",
    "\n",
    "    # Aggregate the weights of selected clients\n",
    "    aggregated_weights = {\n",
    "        key: sum(local_weights[i][key] for i in selected_indices) / num_to_select\n",
    "        for key in local_weights[0].keys()\n",
    "    }\n",
    "    return aggregated_weights\n",
    "\n",
    "\n",
    "# Bulyan Aggregation\n",
    "def bulyan_aggregation(local_weights, num_clients, num_malicious_clients, num_to_select):\n",
    "    selected_updates = []\n",
    "    num_neighbors = num_clients - num_malicious_clients - 2\n",
    "\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "\n",
    "    for _ in range(num_to_select):\n",
    "        scores = []\n",
    "        for i, weight_i in enumerate(flat_weights):\n",
    "            distances = []\n",
    "            for j, weight_j in enumerate(flat_weights):\n",
    "                if i != j:\n",
    "                    distance = torch.norm(weight_i - weight_j, p=2).item()\n",
    "                    distances.append((distance, j))\n",
    "            \n",
    "            distances.sort()\n",
    "            score = sum(d[0] for d in distances[:num_neighbors])\n",
    "            scores.append((score, i))\n",
    "\n",
    "        _, best_client_idx = min(scores)\n",
    "        selected_updates.append(flat_weights[best_client_idx])\n",
    "        flat_weights.pop(best_client_idx)\n",
    "\n",
    "    # Aggregate the selected updates using Trimmed Mean\n",
    "    stacked_updates = torch.stack(selected_updates)\n",
    "    trimmed_updates = torch.mean(stacked_updates, dim=0)\n",
    "    aggregated_weights = reconstruct_weights(trimmed_updates, local_weights[0])\n",
    "    return aggregated_weights\n",
    "\n",
    "# Trimmed Mean Aggregation\n",
    "def trimmed_mean_aggregation(local_weights, trim_ratio):\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "\n",
    "    stacked_updates = torch.stack(flat_weights)\n",
    "    lower_bound = int(trim_ratio * stacked_updates.size(0))\n",
    "    upper_bound = stacked_updates.size(0) - lower_bound\n",
    "\n",
    "    trimmed_updates = torch.mean(torch.sort(stacked_updates, dim=0).values[lower_bound:upper_bound], dim=0)\n",
    "    aggregated_weights = reconstruct_weights(trimmed_updates, local_weights[0])\n",
    "    return aggregated_weights\n",
    "\n",
    "\n",
    "def coomed_aggregation(local_weights):\n",
    "    \"\"\"\n",
    "    Coordinate-wise Median Aggregation\n",
    "    Args:\n",
    "        local_weights (list): List of dictionaries containing model weights for each client.\n",
    "    Returns:\n",
    "        dict: Aggregated global model weights using coordinate-wise median.\n",
    "    \"\"\"\n",
    "    keys = local_weights[0].keys()\n",
    "    aggregated_weights = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        # Stack all client weights for the same parameter key\n",
    "        stacked_weights = torch.stack([local_weights[i][key] for i in range(len(local_weights))])\n",
    "        # Compute coordinate-wise median\n",
    "        aggregated_weights[key] = torch.median(stacked_weights, dim=0).values\n",
    "    \n",
    "    return aggregated_weights\n",
    "\n",
    "def flare_aggregation(local_weights, num_clients):\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "\n",
    "    num_clients = len(local_weights)\n",
    "    mmd_plrs = np.zeros((num_clients, num_clients))\n",
    "    \n",
    "    # Step 2: Compute PLRs (second-to-last layer: weights and biases)\n",
    "    plrs = []\n",
    "    for weights in local_weights:\n",
    "        keys = list(weights.keys())\n",
    "        # Concatenate weight and bias for the second-to-last layer (-4 and -3)\n",
    "        plr = torch.cat(\n",
    "            [weights[keys[-4]].view(-1), weights[keys[-3]].view(-1)]\n",
    "        ).detach().cpu().numpy()\n",
    "        plrs.append(plr)\n",
    "\n",
    "    \n",
    "    # Compute MMD distances between PLRs\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            mmd_plrs[i][j] = mmd_rbf(plrs[i].reshape(1,-1), plrs[j].reshape(1,-1))\n",
    "            mmd_plrs[j][i] = mmd_plrs[i][j]\n",
    "\n",
    "    # Identify nearest neighbors\n",
    "    neigh = np.argsort(mmd_plrs)[:, :5]\n",
    "\n",
    "    # Calculate scaling factors\n",
    "    scale = np.zeros(num_clients)\n",
    "    count_dict = Counter([item for sublist in neigh for item in sublist])\n",
    "    count_exp_sum = sum(np.exp(count) for count in count_dict.values())\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        scale[i] = np.exp(count_dict.get(i, 0)) / count_exp_sum\n",
    "\n",
    "    # Aggregate weights\n",
    "    agg_weights = scale[0] * flat_weights[0].cpu().detach().numpy()\n",
    "    for i in range(1, num_clients):\n",
    "        agg_weights += scale[i] * flat_weights[i].cpu().detach().numpy()\n",
    "    # print(agg_weights)\n",
    "    agg_weights = torch.tensor(agg_weights).to(device)\n",
    "    aggregated_weights = reconstruct_weights(agg_weights, local_weights[0])\n",
    "\n",
    "    return aggregated_weights, count_dict\n",
    "\n",
    "def fltrust_aggregation(local_weights, global_weights):\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "    global_weights = torch.cat([param.view(-1) for param in global_weights.values()]).cpu()\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(num_clients):\n",
    "        score = cosine_similarity(global_weights.reshape(1,-1), flat_weights[i].reshape(1,-1))[0][0]\n",
    "\n",
    "        if np.isnan(score) or score < 0:\n",
    "            similarities.append(0)\n",
    "        else:\n",
    "            similarities.append(score)\n",
    "    # print(similarities)\n",
    "    # Normalize similarities (convert to probabilities)\n",
    "    similarities = np.array(similarities)\n",
    "    similarities = np.exp(similarities)  # Use exp to avoid zero similarities\n",
    "    normalized_similarities = similarities / similarities.sum()\n",
    "    agg_weights = sum(normalized_similarities[i] * flat_weights[i] for i in range(len(flat_weights)))\n",
    "\n",
    "    aggregated_weights = reconstruct_weights(agg_weights, local_weights[0])\n",
    "\n",
    "    return aggregated_weights\n",
    "\n",
    "\n",
    "def fedcc_aggregation(local_weights, glob_plr, method):\n",
    "\n",
    "    # Move flat_weights to the same device (GPU or CPU)\n",
    "    flat_weights = [\n",
    "        torch.cat([param.view(-1) for param in local_weight.values()]).cpu()\n",
    "        for local_weight in local_weights\n",
    "    ]\n",
    "\n",
    "    num_clients = len(local_weights)\n",
    "    glob_plr = glob_plr.detach().cpu().numpy()\n",
    "    similarities = []\n",
    "\n",
    "    # Compute PLRs and move them to the same device\n",
    "    plrs = []\n",
    "    for weights in local_weights:\n",
    "        keys = list(weights.keys())\n",
    "        plr = weights[keys[-4]].detach().cpu().numpy()\n",
    "        plrs.append(plr)\n",
    "    \n",
    "    # Compute similarity values between global PLR and client PLRs\n",
    "    for i in range(len(plrs)):\n",
    "        if method == \"kernel\":\n",
    "            val = kernel_CKA(glob_plr, plrs[i])\n",
    "        elif method == \"linear\":\n",
    "            val = linear_CKA(glob_plr, plrs[i])\n",
    "        elif method == \"mmd\":\n",
    "            val = mmd_rbf(glob_plr, plrs[i])\n",
    "        elif method == \"cosine\":\n",
    "            val = cosine_similarity(glob_plr.reshape(1, -1), plrs[i].reshape(1, -1))[0][0]\n",
    "        elif method == \"euc\":\n",
    "            val = euclidean_distances(glob_plr.reshape(1, -1), plrs[i].reshape(1, -1))[0][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "        similarities.append(0 if np.isnan(val) else val)\n",
    "\n",
    "    print(similarities)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(np.array(similarities).reshape(-1, 1))\n",
    "    labels = kmeans.labels_\n",
    "    counter = Counter(labels)\n",
    "    \n",
    "    # Normalize similarities globally\n",
    "    similarities = np.array(similarities)\n",
    "    similarities = np.exp(similarities)  # Exponentiate to avoid zeros\n",
    "    normalized_similarities = similarities / similarities.sum()\n",
    "\n",
    "    # Identify the larger cluster\n",
    "    larger_cluster = 1 if counter[1] > counter[0] else 0\n",
    "    larger_cluster_members = np.where(labels == larger_cluster)[0]\n",
    "    smaller_cluster_members = np.where(labels != larger_cluster)[0]\n",
    "\n",
    "    # Normalize similarities within the larger cluster\n",
    "    larger_cluster_similarities = normalized_similarities[larger_cluster_members]\n",
    "    larger_cluster_similarities /= larger_cluster_similarities.sum()\n",
    "    \n",
    "    # Aggregation: Ensure tensors are on the correct device\n",
    "    aggregated_flattened_weights = torch.zeros_like(flat_weights[0])\n",
    "\n",
    "    flattened_up_to_second_last = []\n",
    "    flattened_second_last_layer = []\n",
    "    flattened_last_layer = []\n",
    "\n",
    "    # Prepare flattened weights\n",
    "    for local_weight in local_weights:\n",
    "        keys = list(local_weight.keys())\n",
    "        up_to_second_last = torch.cat(\n",
    "            [param.view(-1) for name, param in local_weight.items() if name not in keys[-4:]]\n",
    "        )\n",
    "        second_last_layer = torch.cat(\n",
    "            [local_weight[keys[-4]].view(-1), local_weight[keys[-3]].view(-1)]\n",
    "        )\n",
    "        last_layer = torch.cat(\n",
    "            [local_weight[keys[-2]].view(-1), local_weight[keys[-1]].view(-1)]\n",
    "        )\n",
    "        flattened_up_to_second_last.append(up_to_second_last)\n",
    "        flattened_second_last_layer.append(second_last_layer)\n",
    "        flattened_last_layer.append(last_layer)\n",
    "\n",
    "    # Convert to tensors for easier manipulation and move to the correct device\n",
    "    flattened_up_to_second_last = torch.stack(flattened_up_to_second_last).detach().cpu()\n",
    "    flattened_second_last_layer = torch.stack(flattened_second_last_layer).detach().cpu()\n",
    "    flattened_last_layer = torch.stack(flattened_last_layer).detach().cpu()\n",
    "\n",
    "    # Step 1: Aggregate for all layers except the last two\n",
    "    for idx in range(len(local_weights)):\n",
    "        weight = normalized_similarities[idx]\n",
    "        aggregated_flattened_weights[: flattened_up_to_second_last.shape[1]] += (\n",
    "            weight * flattened_up_to_second_last[idx]\n",
    "        )\n",
    "\n",
    "    # Step 2: Aggregate for the second-to-last layer\n",
    "    start_idx = flattened_up_to_second_last.shape[1]  # Index offset for the second-to-last layer\n",
    "    end_idx = start_idx + flattened_second_last_layer.shape[1]\n",
    "\n",
    "    for idx, client_idx in enumerate(larger_cluster_members):\n",
    "        # Use within-cluster similarity for the second-to-last layer\n",
    "        weight = larger_cluster_similarities[idx]\n",
    "        aggregated_flattened_weights[start_idx:end_idx] += weight * flattened_second_last_layer[client_idx]\n",
    "\n",
    "    # Step 3: Aggregate for the last layer\n",
    "    start_idx = end_idx  # Index offset for the last layer\n",
    "    end_idx = start_idx + flattened_last_layer.shape[1]\n",
    "\n",
    "    for idx in range(len(local_weights)):\n",
    "        weight = normalized_similarities[idx]\n",
    "        aggregated_flattened_weights[start_idx:end_idx] += weight * flattened_last_layer[idx]\n",
    "        \n",
    "    \n",
    "#     for idx, client_idx in enumerate(larger_cluster_members):\n",
    "#         # Use within-cluster similarity for the last layer\n",
    "#         weight = larger_cluster_similarities[idx]\n",
    "#         aggregated_flattened_weights[start_idx:end_idx] += weight * flattened_last_layer[client_idx]\n",
    "\n",
    "    # Step 4: Reconstruct aggregated weights\n",
    "    aggregated_flattened_weights_tensor = aggregated_flattened_weights.to('cuda:0')\n",
    "    aggregated_weights = reconstruct_weights(aggregated_flattened_weights_tensor, local_weights[0])\n",
    "\n",
    "    return aggregated_weights, larger_cluster_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4f1c69-838b-4df7-b8fa-cea540420e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if 'cifar' in dataset:\n",
    "    if dataset == 'cifar':\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_data = datasets.CIFAR10(root=\"../data/cifar10\", train=True, transform=transform, download=True)\n",
    "        test_data = datasets.CIFAR10(root=\"../data/cifar10\", train=False, transform=transform, download=True)\n",
    "        global_model = CNNCifar().to(device)\n",
    "    else:\n",
    "        apply_transform = transforms.Compose(\n",
    "            [transforms.Resize(224),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "        train_data = datasets.CIFAR100(root=\"../data/cifar100\", train=True, download=True, transform=apply_transform)\n",
    "        test_data = datasets.CIFAR100(root=\"../data/cifar100\", train=False, download=True, transform=apply_transform)\n",
    "        global_model = LeNet5().to(device)\n",
    "elif 'mnist' in dataset:\n",
    "    apply_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    if dataset == 'mnist':\n",
    "        data_dir = '../data/mnist/'  \n",
    "        train_data = datasets.MNIST(data_dir, train=True, download=True, transform=apply_transform)\n",
    "        test_data = datasets.MNIST(data_dir, train=False, download=True, transform=apply_transform)\n",
    "\n",
    "    else:\n",
    "        data_dir = '../data/fmnist/'\n",
    "        train_data = datasets.FashionMNIST(data_dir, train=True, download=True, transform=apply_transform)\n",
    "        test_data = datasets.FashionMNIST(data_dir, train=False, download=True, transform=apply_transform)\n",
    "        global_model = CNNFashion_Mnist().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad2609b-f110-44e0-bef9-96f9b5f2d12b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## # Data Preparation\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "#                                  # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "# train_data = datasets.CIFAR10(root=\"../data/cifar10\", train=True, transform=transform, download=True)\n",
    "# test_data = datasets.CIFAR10(root=\"../data/cifar10\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Split data among clients\n",
    "client_data = create_dirichlet_noniid_data(train_data, num_clients, alpha)\n",
    "malicious_clients = random.sample(range(num_clients), num_malicious_clients)\n",
    "print(f\"Malicious client idx {malicious_clients}\")\n",
    "\n",
    "# DataLoader for test data\n",
    "clean_global_test_loader, backdoor_global_test_loader = create_test_datasets(test_data, backdoor_label, trigger_value)\n",
    "\n",
    "# Create clean and backdoor test datasets for each client\n",
    "local_test_datasets = []\n",
    "for client_idx in range(num_clients):\n",
    "    clean_test_loader, backdoor_test_loader = create_local_test_datasets(test_data, backdoor_label, trigger_value)\n",
    "    local_test_datasets.append((clean_test_loader, backdoor_test_loader))\n",
    "\n",
    "clean_acc_history = []\n",
    "backdoor_acc_history = []\n",
    "confidence = []    \n",
    "\n",
    "# Federated Training\n",
    "# global_model = CNNCifar().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    local_weights = []\n",
    "    for client_idx, client_dataset in enumerate(client_data):\n",
    "        if 'cifar' in dataset:\n",
    "            if dataset == 'cifar':\n",
    "                local_model = CNNCifar().to(device)\n",
    "            else:\n",
    "                local_model = LeNet5().to(device)\n",
    "        elif 'mnist' in dataset:\n",
    "                local_model = CNNFashion_Mnist().to(device)\n",
    "\n",
    "                # local_model = CNNCifar().to(device)  # Move local model to GPU\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        # Set local epochs and learning rate\n",
    "        if client_idx in malicious_clients and epoch < switch_epoch:\n",
    "            local_epochs = 6\n",
    "            learning_rate = 0.05\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.005)\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                 milestones=[0.2 * local_epochs,\n",
    "                                                             0.8 * local_epochs], gamma=0.1)\n",
    "        else:\n",
    "            local_epochs = 2\n",
    "            learning_rate = 0.1\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                 milestones=[0.2 * local_epochs,\n",
    "                                                             0.8 * local_epochs], gamma=0.1)\n",
    "\n",
    "        pos = 0\n",
    "\n",
    "        # Malicious clients inject backdoor samples\n",
    "        if client_idx in (malicious_clients) and epoch < switch_epoch:\n",
    "            x, y = zip(*client_dataset)\n",
    "            x = torch.stack(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "            data_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
    "            x_batches, y_batches = [], []\n",
    "            for x_batch, y_batch in data_loader:\n",
    "                if len(x_batch) >= backdoor_samples_per_batch:\n",
    "                    x_batch, y_batch = inject_backdoor_samples(\n",
    "                        x_batch, y_batch, backdoor_samples_per_batch, backdoor_label, pos\n",
    "                    )\n",
    "                x_batches.append(x_batch)\n",
    "                y_batches.append(y_batch)\n",
    "            x = torch.cat(x_batches).to(device)\n",
    "            y = torch.cat(y_batches).to(device)\n",
    "            data_loader = DataLoader(list(zip(x, y)), batch_size=batch_size, shuffle=True)\n",
    "            pos = pos + 1\n",
    "        else:\n",
    "            data_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Train the local model\n",
    "        local_model.train()\n",
    "        for local_epoch in range(local_epochs):\n",
    "            for x_batch, y_batch in data_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = local_model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Collect local weights\n",
    "        weight_update = {key: val.clone() for key, val in local_model.state_dict().items()}\n",
    "\n",
    "        # Apply boosting factor for malicious clients during early epochs\n",
    "        if client_idx in malicious_clients and epoch < switch_epoch:\n",
    "            weight_update = {\n",
    "                key: weight_update[key] * scale_factor\n",
    "                for key in weight_update.keys()\n",
    "            }\n",
    "\n",
    "        local_weights.append(weight_update)\n",
    "\n",
    "        # Evaluate the local model on its clean and backdoor test datasets\n",
    "        clean_test_loader, backdoor_test_loader = local_test_datasets[client_idx]\n",
    "        clean_accuracy, _ = evaluate_model(local_model, clean_test_loader, device)\n",
    "        backdoor_accuracy, _ = evaluate_model(local_model, backdoor_test_loader, device)\n",
    "\n",
    "        print(f\"Client {client_idx} - Clean Accuracy: {clean_accuracy:.4f} | Backdoor Accuracy: {backdoor_accuracy:.4f}\")\n",
    "\n",
    "        \n",
    "    # # Choose aggregation rule based on the current epoch\n",
    "    if aggregation_rule == \"fedavg\" or epoch < switch_epoch:\n",
    "        print(\"Using FedAvg with boosted malicious updates for this epoch.\")\n",
    "        # FedAvg: Simple averaging of local weights (with boosted malicious updates)\n",
    "        global_state_dict = {\n",
    "            key: sum(local_weights[i][key] for i in range(len(local_weights))) / len(local_weights)\n",
    "            for key in local_weights[0].keys()\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Using {aggregation_rule.capitalize()} for this epoch.\")\n",
    "        # Use the selected aggregation rule\n",
    "        if aggregation_rule == \"krum\":\n",
    "            global_state_dict = krum_aggregation(local_weights, num_clients, num_malicious_clients)\n",
    "        elif aggregation_rule == \"multi-krum\":\n",
    "            num_to_select = num_clients - num_malicious_clients - 2  # Hyperparameter for Multi-Krum\n",
    "            global_state_dict = multi_krum_aggregation(local_weights, num_clients, num_malicious_clients, num_to_select)\n",
    "        elif aggregation_rule == \"bulyan\":\n",
    "            num_to_select = num_clients - 2 * num_malicious_clients\n",
    "            global_state_dict = bulyan_aggregation(local_weights, num_clients, num_malicious_clients, num_to_select)\n",
    "        elif aggregation_rule == \"trimmed_mean\":\n",
    "            global_state_dict = trimmed_mean_aggregation(local_weights, trim_ratio)\n",
    "        elif aggregation_rule == \"coomed\":\n",
    "            global_state_dict = coomed_aggregation(local_weights)\n",
    "        elif aggregation_rule == \"flare\":\n",
    "            global_state_dict, _ = flare_aggregation(local_weights, num_clients)\n",
    "        elif aggregation_rule == \"fltrust\":\n",
    "            global_state_dict = fltrust_aggregation(local_weights, global_model.state_dict())\n",
    "        elif aggregation_rule == \"fedcc\":\n",
    "            glob_weights = global_model.state_dict()\n",
    "            keys = list(glob_weights.keys())\n",
    "            glob_plr = glob_weights[keys[-4]]\n",
    "            global_state_dict, _ = fedcc_aggregation(local_weights, glob_plr, method)\n",
    "\n",
    "    # Load the updated global weights into the global model\n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "\n",
    "    # Evaluate global model on clean and backdoor test datasets\n",
    "    clean_accuracy, _ = evaluate_model(global_model, clean_global_test_loader, device)\n",
    "    backdoor_accuracy, mal_out = evaluate_model(global_model, backdoor_global_test_loader, device)\n",
    "    print(f\"{dataset}, boost={scale_factor}, alpha={alpha}\")\n",
    "    print(f\"Global Clean Accuracy: {clean_accuracy:.4f}\")\n",
    "    print(f\"Global Backdoor Accuracy: {backdoor_accuracy:.4f}\")\n",
    "    \n",
    "    clean_acc_history.append(clean_accuracy)\n",
    "    backdoor_acc_history.append(backdoor_accuracy)\n",
    "    confidence.append(mal_out)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "data = {\n",
    "    \"clean_acc\": clean_acc_history,\n",
    "    \"backdoor_acc\": backdoor_acc_history,\n",
    "    \"confidence\": confidence\n",
    "}\n",
    "csv_file_path = f\"output/confidence_dba_{alpha}_{dataset}_{aggregation_rule}.csv\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3bf56-fa3e-4803-897d-febce8a3effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, epochs+1))  # 40 epochs\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, clean_acc_history, label='Clean Accuracy', marker='o', color='blue')\n",
    "plt.plot(epochs, backdoor_acc_history, label='Backdoor Accuracy', marker='x', color='red')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(f'{aggregation_rule} starts from {switch_epoch}', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14ac8a-830a-460a-a254-46a47ff89068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Data Preparation\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "#                                  # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "# train_data = datasets.CIFAR10(root=\"../data/cifar10\", train=True, transform=transform, download=True)\n",
    "# test_data = datasets.CIFAR10(root=\"../data/cifar10\", train=False, transform=transform, download=True)\n",
    "\n",
    "# # Split data among clients\n",
    "# client_data = create_dirichlet_noniid_data(train_data, num_clients, alpha)\n",
    "# malicious_clients = random.sample(range(num_clients), num_malicious_clients)\n",
    "# print(f\"Malicious client idx {malicious_clients}\")\n",
    "\n",
    "# # DataLoader for test data\n",
    "# clean_global_test_loader, backdoor_global_test_loader = create_test_datasets(test_data, backdoor_label, trigger_value)\n",
    "\n",
    "# # Create clean and backdoor test datasets for each client\n",
    "# local_test_datasets = []\n",
    "# for client_idx in range(num_clients):\n",
    "#     clean_test_loader, backdoor_test_loader = create_local_test_datasets(test_data, backdoor_label, trigger_value)\n",
    "#     local_test_datasets.append((clean_test_loader, backdoor_test_loader))\n",
    "\n",
    "# # Federated Training\n",
    "# global_model = CNNCifar().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "#     local_weights = []\n",
    "#     for client_idx, client_dataset in enumerate(client_data):\n",
    "#         local_model = CNNCifar().to(device)  # Move local model to GPU\n",
    "#         local_model.load_state_dict(global_model.state_dict())\n",
    "        \n",
    "#         # Set local epochs and learning rate\n",
    "#         if client_idx in malicious_clients and epoch < 70:\n",
    "#             local_epochs = 6\n",
    "#             learning_rate = 0.05\n",
    "#             optimizer = optim.SGD(local_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.005)\n",
    "#             scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#                                                  milestones=[0.2 * local_epochs,\n",
    "#                                                              0.8 * local_epochs], gamma=0.1)\n",
    "#         else:\n",
    "#             local_epochs = 2\n",
    "#             learning_rate = 0.1\n",
    "#             optimizer = optim.SGD(local_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "                        \n",
    "#         pos = 0\n",
    "\n",
    "#         # Malicious clients inject backdoor samples\n",
    "#         if client_idx in (malicious_clients) and epoch < 70:\n",
    "#             x, y = zip(*client_dataset)\n",
    "#             x = torch.stack(x).to(device)\n",
    "#             y = torch.tensor(y).to(device)\n",
    "#             data_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
    "#             x_batches, y_batches = [], []\n",
    "#             for x_batch, y_batch in data_loader:\n",
    "#                 if len(x_batch) >= backdoor_samples_per_batch:\n",
    "#                     x_batch, y_batch = inject_backdoor_samples(\n",
    "#                         x_batch, y_batch, backdoor_samples_per_batch, backdoor_label, pos\n",
    "#                     )\n",
    "#                 x_batches.append(x_batch)\n",
    "#                 y_batches.append(y_batch)\n",
    "#             x = torch.cat(x_batches).to(device)\n",
    "#             y = torch.cat(y_batches).to(device)\n",
    "#             data_loader = DataLoader(list(zip(x, y)), batch_size=batch_size, shuffle=True)\n",
    "#             pos = pos + 1\n",
    "#         else:\n",
    "#             data_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         # Train the local model\n",
    "#         local_model.train()\n",
    "#         for local_epoch in range(local_epochs):\n",
    "#             for x_batch, y_batch in data_loader:\n",
    "#                 x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = local_model(x_batch)\n",
    "#                 loss = criterion(outputs, y_batch)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#         # Save the local weights\n",
    "#         local_weights.append({key: val.clone() for key, val in local_model.state_dict().items()})\n",
    "\n",
    "#         # Evaluate the local model on its clean and backdoor test datasets\n",
    "#         clean_test_loader, backdoor_test_loader = local_test_datasets[client_idx]\n",
    "#         clean_accuracy = evaluate_model(local_model, clean_test_loader, device)\n",
    "#         backdoor_accuracy = evaluate_model(local_model, backdoor_test_loader, device)\n",
    "\n",
    "#         print(f\"Client {client_idx} - Clean Accuracy: {clean_accuracy:.4f} | Backdoor Accuracy: {backdoor_accuracy:.4f}\")\n",
    "\n",
    "#     # Federated averaging\n",
    "#     global_state_dict = global_model.state_dict()\n",
    "#     for key in global_state_dict.keys():\n",
    "#         # Scale the malicious clients' weights before averaging\n",
    "#         scaled_weights = []\n",
    "#         for i, local_weight in enumerate(local_weights):\n",
    "#             if i in malicious_clients and epoch < 70:\n",
    "#                 scaled_weights.append(scale_factor * local_weight[key])  # Scale malicious client weights\n",
    "#             else:\n",
    "#                 scaled_weights.append(local_weight[key])  # No scaling for benign clients\n",
    "        \n",
    "#         # Perform averaging of all client weights (scaled for malicious clients)\n",
    "#         global_state_dict[key] = torch.mean(torch.stack(scaled_weights), dim=0)\n",
    "\n",
    "#     # Load the updated global weights into the global model\n",
    "#     global_model.load_state_dict(global_state_dict)\n",
    "\n",
    "#     # Evaluate global model on clean and backdoor test datasets\n",
    "#     clean_accuracy = evaluate_model(global_model, clean_global_test_loader, device)\n",
    "#     backdoor_accuracy = evaluate_model(global_model, backdoor_global_test_loader, device)\n",
    "#     print(f\"Global Clean Accuracy: {clean_accuracy:.4f}\")\n",
    "#     print(f\"Global Backdoor Accuracy: {backdoor_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a68e3-114f-4cf0-8a06-598bcf81a147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ba46a-c91f-4a71-b23d-59bb6c7275ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97fcec-679e-44ea-bb13-228bb45ea462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a5750-f11a-4c7c-8386-207a2a6cfc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4f98-bf95-4f25-a71a-908fc12b55e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976c3d3-942f-4d3c-932a-7616e61fd773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4dd47-cc98-4de5-b87f-aa4b7583374c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
