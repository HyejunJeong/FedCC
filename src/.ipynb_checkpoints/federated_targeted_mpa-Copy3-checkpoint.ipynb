{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ff8712-121e-4c55-9b57-eb317f36a195",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plan\n",
    "\n",
    "- [x] FL base framework\n",
    "- [x] implement non-iid sampling function following dirichlet dist\n",
    "\n",
    "#### Attack simulation\n",
    "- [x] untargeted model poisoning\n",
    "    - Local model poisoning attacks to Byzantine-robust federated learning <br>\n",
    "    https://github.com/vrt1shjwlkr/NDSS21-Model-Poisoning fang attack <br>\n",
    "    krum-attack, trimmed-mean/median attack\n",
    "- [x] targeted model poisoning\n",
    "    - Analyzing federated learning through an adversarial lens <br>\n",
    "    https://github.com/inspire-group/ModelPoisoning\n",
    "- [x] data poisoning\n",
    "    - DBA: Distributed backdoor attacks against federated learning <br>\n",
    "    https://github.com/AI-secure/DBA\n",
    "    \n",
    "#### Defense baseline\n",
    "- [x] FedAvg\n",
    "- [x] Krum\n",
    "- [x] Multi-Krum\n",
    "- [x] Bulyan\n",
    "- [x] Coordinate median\n",
    "- [x] FLARE\n",
    "\n",
    "#### Proposed method\n",
    "- [x] extract PLRs\n",
    "- [x] apply RBF hypersphere CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97834e9-666d-4e4a-a15e-332181ee70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e5affa-9f28-4dad-99af-52f285107603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, mal_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, Alexnet, modelC, LeNet5\n",
    "from utils import get_dataset, get_mal_dataset, exp_details, flatten, construct_ordered_dict\n",
    "from aggregate import fedavg, multi_krum, krum, coomed, bulyan, tr_mean, fed_align, fed_cc, flare, fltrust\n",
    "from attacks import get_malicious_updates_untargeted_mkrum, get_malicious_updates_untargeted_med, get_malicious_updates_targeted\n",
    "from cka import linear_CKA, kernel_CKA\n",
    "# python src/federated_main.py --model=cnn --dataset=cifar --gpu=0 --iid=1 --epochs=10\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5ee40-5071-437b-bc8e-35135865c450",
   "metadata": {},
   "source": [
    "# Targeted Model Poisoning Attack (label flipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea6ac2c-32ca-417f-90fb-f5b58b7ad1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 50\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 3 # num of local epoch\n",
    "    local_bs = 100 # batch size\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    aggregation = 'fedcc' # trmean, flare, fltrust, fedcc fedavg, krum, mkrum, trmean, coomed, bulyan, flare, fedcc, fltrust\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'cifar' # fmnist, cifar, mnist\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes = 100 \n",
    "        num_channels = 3 # num of channels of imgs\n",
    "    else:\n",
    "        num_classes = 10\n",
    "        num_channels = 1\n",
    "    \n",
    "    gpu = 0\n",
    "    optimizer = 'adam'\n",
    "    iid = 0 # 0 for non-iid\n",
    "    alpha = 0.2 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_clients = [0] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 5 # number of maliciuos data sample\n",
    "    mal_bs = 100\n",
    "    mal_lr = 0.005\n",
    "    mal_test_bs = 100\n",
    "    local_mal_ep = 6\n",
    "    boost = 5 # alpha: 2 for fedavg, 3.5 for krum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462f137-23d4-4d56-8de6-1012b0181ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : cnn\n",
      "    Optimizer : adam\n",
      "    Learning  : 0.001\n",
      "    Aggregation     : fedcc\n",
      "    Global Rounds   : 50\n",
      "\n",
      "    Federated parameters:\n",
      "    Non-IID              : 0.2\n",
      "    Fraction of users    : 1\n",
      "    Local Batch size     : 100\n",
      "    Local Epochs         : 3\n",
      "\n",
      "    Malicious parameters:\n",
      "    Attackers            : [0]\n",
      "    Attack Type          : targeted\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CNNCifar(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "malcious dataset true labels: 9, malicious labels: [7, 7, 7, 7, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "| Global Training Round : 1 |\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjeong_umass_edu/.conda/envs/torchgpu/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0, loss 0.9027448676250599, acc 29.1970802919708, mal loss 2.9416537284851074, mal acc 0.0\n",
      "user 1, loss 0.8725316736612235, acc 88.62144420131291\n",
      "user 2, loss 0.9857898461070039, acc 78.57142857142857\n",
      "user 3, loss 0.25065938551480754, acc 100.0\n",
      "user 4, loss 0.4986122154681258, acc 21.2406015037594\n",
      "user 5, loss 1.3994541984420639, acc 62.745098039215684\n",
      "user 6, loss 0.121869338903232, acc 100.0\n",
      "user 7, loss 0.4347316640796084, acc 89.94515539305301\n",
      "user 8, loss 1.4066359549760818, acc 16.853932584269664\n",
      "user 9, loss 0.9929874895984291, acc 59.896907216494846\n",
      "fed_cc Selected idx: [1 3 4 5 7 8]\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 0.7866016634375634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:18<15:30, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 13.27% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 3.07, confidence: 0.04662033319473267\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 2 |\n",
      "=========================================\n",
      "user 0, loss 0.8605722569757037, acc 15.875912408759124, mal loss 3.4329447746276855, mal acc 0.0\n",
      "user 1, loss 0.7549328693935463, acc 90.15317286652079\n",
      "user 2, loss 0.9367183501593733, acc 86.18266978922716\n",
      "user 3, loss 0.19787840884274396, acc 100.0\n",
      "user 4, loss 0.387427045162334, acc 22.55639097744361\n",
      "user 5, loss 1.2563471820977357, acc 69.49891067538127\n",
      "user 6, loss 0.1032634203884543, acc 100.0\n",
      "user 7, loss 0.3483794136255076, acc 83.72943327239489\n",
      "user 8, loss 1.1677336767315865, acc 65.1685393258427\n",
      "user 9, loss 0.9588809746962327, acc 73.4020618556701\n",
      "fed_cc Selected idx: [1 3 4 5 7 8]\n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training Loss : 0.7419075116224426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:38<15:22, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 19.82% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 3.81, confidence: 0.02302250862121582\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 3 |\n",
      "=========================================\n",
      "user 0, loss 0.8053478566584763, acc 18.065693430656935, mal loss 3.9732792377471924, mal acc 0.0\n",
      "user 1, loss 0.6988836416790077, acc 89.71553610503283\n",
      "user 2, loss 0.8818500090912345, acc 73.18501170960188\n",
      "user 3, loss 0.1500876922494708, acc 100.0\n",
      "user 4, loss 0.29000735346430034, acc 21.2406015037594\n",
      "user 5, loss 1.1754199336240958, acc 90.6318082788671\n",
      "user 6, loss 0.07057735329390284, acc 100.0\n",
      "user 7, loss 0.3084726519318241, acc 73.6745886654479\n",
      "user 8, loss 1.0005620097120602, acc 59.55056179775281\n",
      "user 9, loss 0.8978177833760905, acc 67.5257731958763\n",
      "fed_cc Selected idx: [1 3 4 5 7 8]\n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training Loss : 0.7039058839176437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:55<14:27, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 19.87% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.50, confidence: 0.012015944719314576\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 4 |\n",
      "=========================================\n",
      "user 0, loss 0.7696266408871723, acc 16.970802919708028, mal loss 3.888416290283203, mal acc 0.0\n",
      "user 1, loss 0.6564105169193165, acc 88.40262582056893\n",
      "user 2, loss 0.8216652173350975, acc 66.39344262295081\n",
      "user 3, loss 0.11728605778458027, acc 100.0\n",
      "user 4, loss 0.2609218895550846, acc 22.932330827067666\n",
      "user 5, loss 1.0964111964981835, acc 88.45315904139433\n",
      "user 6, loss 0.0552019612827003, acc 100.0\n",
      "user 7, loss 0.27671137141684693, acc 75.3199268738574\n",
      "user 8, loss 1.0038829421003659, acc 61.79775280898876\n",
      "user 9, loss 0.8771288321058974, acc 82.2680412371134\n",
      "fed_cc Selected idx: [1 3 4 5 7 8]\n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training Loss : 0.6763105785853639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:13<14:01, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 25.11% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 4.85, confidence: 0.008684533834457397\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 5 |\n",
      "=========================================\n",
      "user 0, loss 0.7223910975235479, acc 20.072992700729927, mal loss 3.5225791931152344, mal acc 0.0\n",
      "user 1, loss 0.6089117607554874, acc 88.18380743982495\n",
      "user 2, loss 0.7636014388667212, acc 82.31850117096019\n",
      "user 3, loss 0.10722213799435383, acc 100.0\n",
      "user 4, loss 0.24222581694985543, acc 22.932330827067666\n",
      "user 5, loss 1.0312771421295028, acc 91.06753812636165\n",
      "user 6, loss 0.03595870502378694, acc 100.0\n",
      "user 7, loss 0.2647716853428971, acc 81.53564899451554\n",
      "user 8, loss 0.8967272092898687, acc 52.80898876404494\n",
      "user 9, loss 0.8483069016892685, acc 80.10309278350516\n",
      "fed_cc Selected idx: [1 3 4 5 7 8]\n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training Loss : 0.6514763407795969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:34<14:13, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global model Benign Test Accuracy: 27.70% \n",
      "Global model Malicious Accuracy: 0.00%, Malicious Loss: 5.32, confidence: 0.006213701143860817\n",
      "\n",
      "=========================================\n",
      "| Global Training Round : 6 |\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if args.gpu == 0 else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "        elif args.dataset == 'cifar100':\n",
    "            global_model = LeNet5(args=args)\n",
    "\n",
    "            \n",
    "    # elif args.model == 'alexnet':\n",
    "    #     if args.dataset == 'cifar100':\n",
    "    #         global_model = Alexnet(args=args)\n",
    "    \n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "        print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    \n",
    "    confidence = []    \n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "            \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_clients:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            else:\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            # boost the malicious (weight+bias) by alpha\n",
    "            # if mal_user:\n",
    "            #     flat_delta_m = flatten(w) - flatten(w_prev)\n",
    "            #     flat_mal_w = flatten(w_prev) + args.boost * flat_delta_m\n",
    "            #     w = construct_ordered_dict(global_model, torch.tensor(flat_mal_w).to(device))\n",
    "              \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "            \n",
    "            # if agg==krum, alternate training\n",
    "            if args.aggregation == 'krum':\n",
    "                if idx in args.mal_clients and epoch %2 ==0:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else:                     \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "            else:\n",
    "                # if malicious user: boost the detlta weights\n",
    "                if idx in args.mal_clients:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else: \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_clients)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif args.aggregation == 'fltrust':\n",
    "            glob_weights = []\n",
    "            glob_weights.append(flatten(global_weights))\n",
    "            agg_weights = fltrust(malicious_grads, glob_weights)\n",
    "\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]\n",
    "            agg_weights, selected_idxs = fed_cc(local_weights, glob_plr, 'kernel')\n",
    "            print(f'fed_cc Selected idx: {selected_idxs}')\n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "            \n",
    "            if len(args.mal_clients) > 0:\n",
    "                mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "                print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "                confidence.append(mal_out)\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    \n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "        print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out))\n",
    "    \n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    data = {\n",
    "        \"clean_acc\": test_acc,\n",
    "        \"backdoor_acc\": mal_acc,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "    csv_file_path = f\"output/confidence_{args.alpha}_{args.dataset}_{args.aggregation}.csv\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20bde6-2276-4d49-803f-2e64717e58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cifar trmean 1 fmnist trmean\n",
    "\n",
    "class Args(object):\n",
    "    \n",
    "    # federated parameters (default values are set)\n",
    "    epochs = 50\n",
    "    num_users = 10\n",
    "    frac = 1 # fraction of clients\n",
    "    local_ep = 3 # num of local epoch\n",
    "    local_bs = 100 # batch size\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    aggregation = 'fedcc' # fedavg, krum, mkrum, coomed, bulyan, flare, fedcc\n",
    "    \n",
    "    # model arguments\n",
    "    model = 'cnn'\n",
    "    kernel_num = 9 # num of each kind of kernel\n",
    "    kernel_sizes = '3,4,5' # comma-separated kernel size to use for convolution\n",
    "    norm = 'batch_norm' # batch_norm, layer_norm, None\n",
    "    num_filters = 32 # num of filters for conv nets -- 32 for mini-imagenet, 64 for omiglot\n",
    "    max_pool = 'True' # whether use max pooling rather than strided convolutions\n",
    "    \n",
    "    # other arguments\n",
    "    dataset = 'cifar' # fmnist, cifar, mnist\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes = 100 \n",
    "        num_channels = 3 # num of channels of imgs\n",
    "    else:\n",
    "        num_classes = 10\n",
    "        num_channels = 1\n",
    "    \n",
    "    gpu = 0\n",
    "    optimizer = 'adam'\n",
    "    iid = 0 # 0 for non-iid\n",
    "    alpha = 0.2 # noniid --> (0, 1] <-- iid\n",
    "    unequal = 0 # whether to use unequal data splits for non-iid settings (0 for equal splits)\n",
    "    stopping_rounds = 10 # rounds of early stopping\n",
    "    verbose = 0\n",
    "    seed = 1\n",
    "\n",
    "    # malicious arguments\n",
    "    mal_clients = [0] # indices of malicious user\n",
    "    attack_type = 'targeted' # targeted\n",
    "    num_mal = 5 # number of maliciuos data sample\n",
    "    mal_bs = 100\n",
    "    mal_lr = 0.005\n",
    "    mal_test_bs = 100\n",
    "    local_mal_ep = 6\n",
    "    boost = 5 # alpha: 2 for fedavg, 3.5 for krum\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # define paths\n",
    "    path_project = os.path.abspath('..')\n",
    "    logger = SummaryWriter('../logs')\n",
    "\n",
    "    args = Args()\n",
    "    exp_details(args)\n",
    "\n",
    "    device = 'cuda:0' if args.gpu == 0 else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    # for n_attacker in args.n_attackers:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if args.model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if args.dataset == 'mnist':\n",
    "            global_model = CNNMnist(args=args)\n",
    "        elif args.dataset == 'fmnist':\n",
    "            global_model = CNNFashion_Mnist(args=args)\n",
    "        elif args.dataset == 'cifar':\n",
    "            global_model = CNNCifar(args=args)\n",
    "        elif args.dataset == 'cifar100':\n",
    "            global_model = LeNet5(args=args)\n",
    "\n",
    "            \n",
    "    # elif args.model == 'alexnet':\n",
    "    #     if args.dataset == 'cifar100':\n",
    "    #         global_model = Alexnet(args=args)\n",
    "    \n",
    "    elif args.model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_X_list, mal_Y, Y_true = get_mal_dataset(test_dataset, args.num_mal, args.num_classes)\n",
    "        print(\"malcious dataset true labels: {}, malicious labels: {}\".format(Y_true, mal_Y))\n",
    "\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    print_every = 1\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    \n",
    "    confidence = []    \n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'| Global Training Round : {epoch+1} |')\n",
    "        print('=========================================')\n",
    "\n",
    "        global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "        # flattened weights + bias\n",
    "        flattened_local_weights = []\n",
    "        \n",
    "        # create separate arrays for weights and biases and the offsets \n",
    "        only_weights = [] # for krum to consider only weights, not the biases\n",
    "        only_biases = []\n",
    "            \n",
    "        for idx in range(args.num_users):\n",
    "            mal_user = False\n",
    "            \n",
    "            # alternating benign and malicious training \n",
    "            if idx in args.mal_clients:# and epoch % 2 == 0:\n",
    "                mal_user = True\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger, \\\n",
    "                                      mal=mal_user, mal_X=mal_X_list, mal_Y=mal_Y, test_dataset=test_dataset)\n",
    "            else:\n",
    "                local_model = LocalUpdate(args=args, dataset=train_dataset, idxs=user_groups[idx], logger=logger)\n",
    "                \n",
    "            w_prev = global_model.state_dict()\n",
    "            w, loss = local_model.update_weights(model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            # boost the malicious (weight+bias) by alpha\n",
    "            # if mal_user:\n",
    "            #     flat_delta_m = flatten(w) - flatten(w_prev)\n",
    "            #     flat_mal_w = flatten(w_prev) + args.boost * flat_delta_m\n",
    "            #     w = construct_ordered_dict(global_model, torch.tensor(flat_mal_w).to(device))\n",
    "              \n",
    "            # construct two arrays with weights-only and bias-only\n",
    "            if 'krum' in args.aggregation:\n",
    "                for key in w:\n",
    "                    if 'weight' in key:\n",
    "                        if mal_user and epoch % 2 == 0:\n",
    "                            only_weights = np.append(only_weights, args.boost * w[key].detach().cpu().numpy().reshape(-1))\n",
    "                        else:\n",
    "                            only_weights = np.append(only_weights, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                    elif 'bias' in key:\n",
    "                        only_biases = np.append(only_biases, w[key].detach().cpu().numpy().reshape(-1))\n",
    "                            \n",
    "            new_model = copy.deepcopy(global_model)\n",
    "            new_model.load_state_dict(w)\n",
    "            acc, _ = local_model.inference(model=new_model)\n",
    "\n",
    "            if mal_user == True:\n",
    "                mal_acc, mal_loss = local_model.mal_inference(model=new_model)\n",
    "                print('user {}, loss {}, acc {}, mal loss {}, mal acc {}'.format(idx, loss, 100*acc, mal_loss, 100*mal_acc))\n",
    "            else:\n",
    "                print('user {}, loss {}, acc {}'.format(idx, loss, 100*acc))\n",
    "\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "            \n",
    "            # if agg==krum, alternate training\n",
    "            if args.aggregation == 'krum':\n",
    "                if idx in args.mal_clients and epoch %2 ==0:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else:                     \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "            else:\n",
    "                # if malicious user: boost the detlta weights\n",
    "                if idx in args.mal_clients:\n",
    "                    flattened_local_weights.append(flatten(w_prev) + args.boost * (flatten(w) - flatten(w_prev)))\n",
    "                else: \n",
    "                    flattened_local_weights.append(flatten(w))\n",
    "        \n",
    "        only_weights = torch.tensor(np.array(only_weights)).to(device)\n",
    "                \n",
    "        flattened_local_weights = torch.tensor(np.array(flattened_local_weights)).to(device)\n",
    "        malicious_grads = flattened_local_weights\n",
    "\n",
    "        n_attacker = len(args.mal_clients)\n",
    "        \n",
    "        \n",
    "        # update global weights\n",
    "        if args.aggregation == 'fedavg':\n",
    "            agg_weights = fedavg(malicious_grads)\n",
    "        elif args.aggregation == 'krum':\n",
    "            agg_weights, selected_idxs = krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'Krum Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'mkrum':\n",
    "            agg_weights, selected_idxs = multi_krum(malicious_grads, n_attacker, only_weights=only_weights)\n",
    "            print(f'multiKrum Selected idxs: {selected_idxs}')\n",
    "        elif args.aggregation == 'coomed':\n",
    "            agg_weights = coomed(malicious_grads)\n",
    "            print(f'\\ndiff {torch.norm((agg_weights - flattened_local_weights[0])) ** 2}')\n",
    "        elif args.aggregation == 'bulyan':\n",
    "            agg_weights, selected_idxs = bulyan(malicious_grads, n_attacker)\n",
    "            print(f'Bulyan Selected idx: {selected_idxs}')\n",
    "        elif args.aggregation == 'trmean':\n",
    "            agg_weights = tr_mean(malicious_grads, n_attacker)\n",
    "            \n",
    "        elif args.aggregation == 'fltrust':\n",
    "            glob_weights = []\n",
    "            glob_weights.append(flatten(global_weights))\n",
    "            agg_weights = fltrust(malicious_grads, glob_weights)\n",
    "\n",
    "        elif args.aggregation == 'flare':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer]) for each_local in structured_local_weights]\n",
    "            agg_weights, count_dict = flare(malicious_grads, plrs)\n",
    "            print(f'flare count_dict: {count_dict}')\n",
    "\n",
    "        elif args.aggregation == 'fedcc':\n",
    "            second_last_layer = list(local_weights[0].keys())[-4]\n",
    "            glob_plr = global_weights[second_last_layer]\n",
    "            structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "            plrs = [(each_local[second_last_layer].reshape((glob_plr.shape[0], glob_plr.shape[1]))) for each_local in structured_local_weights]\n",
    "            agg_weights, selected_idxs = fed_cc(local_weights, glob_plr, 'kernel')\n",
    "            print(f'fed_cc Selected idx: {selected_idxs}')\n",
    "        else:\n",
    "            raise ValueError('Unknown aggregation strategy: {}'.format(args.aggregation))\n",
    "\n",
    "\n",
    "        # reshape the flattened global weights into the ordereddict\n",
    "        global_weights = construct_ordered_dict(global_model, agg_weights)\n",
    "        \n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {epoch+1} global rounds:')\n",
    "            print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "            \n",
    "            test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "            print('\\nGlobal model Benign Test Accuracy: {:.2f}% '.format(100*test_acc))\n",
    "            \n",
    "            if len(args.mal_clients) > 0:\n",
    "                mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "                print('Global model Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence: {}\\n'.format(100*mal_acc, mal_loss, mal_out))\n",
    "                confidence.append(mal_out)\n",
    "                \n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "\n",
    "    print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "    print(\"|---- Test Benign Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "    \n",
    "    if len(args.mal_clients) > 0:\n",
    "        mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, mal_X_list, mal_Y)\n",
    "        print(\"|---- Test Malicious Accuracy: {:.2f}%, Malicious Loss: {:.2f}, confidence:{}\\n\".format(100*mal_acc, mal_loss, mal_out))\n",
    "    \n",
    "    print('\\n Total Run Time: {0:0.4f}'.format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    data = {\n",
    "        \"clean_acc\": test_acc,\n",
    "        \"backdoor_acc\": mal_acc,\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "    csv_file_path = f\"output/confidence_{args.alpha}_{args.dataset}_{args.aggregation}.csv\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeef1b3-2a10-4cbf-af1c-55bd83b7f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda3a20-2321-4637-912f-1cf8e2b6dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_weights = []\n",
    "for key in global_weights:\n",
    "    if 'weight' in key:\n",
    "        only_weights = np.append(only_weights, global_weights[key].detach().cpu().numpy())\n",
    "_, glob_svd, _, = np.linalg.svd(only_weights.reshape(1,-1))\n",
    "glob_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b17f74-9b4f-40eb-886f-1000bf52b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('euclidean')\n",
    "for i in range(len(plrs)):\n",
    "    val = euclidean_distances(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    # print(val)\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63086320-b1f9-46ff-aed0-34819c44ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, paired_cosine_distances\n",
    "\n",
    "sim_vals = []\n",
    "glob_plr = glob_plr.detach().cpu()\n",
    "\n",
    "print('cosine')\n",
    "for i in range(len(plrs)):\n",
    "    val = cosine_similarity(glob_plr.reshape(1, -1), plrs[i].detach().cpu().reshape(1,-1))[0][0]\n",
    "    if np.isnan(val):\n",
    "        sim_vals.append(0)\n",
    "    else:\n",
    "        sim_vals.append(val)\n",
    "print(sim_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d998913-cbd5-4897-92f2-3d5bb6661a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5079-8318-48dc-9c60-de72aeef88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv1_weights = np.array(conv1_weights)\n",
    "conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv2_weights = np.array(conv2_weights)\n",
    "conv3_weights = [np.array(w['conv3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "conv3_weights = np.array(conv3_weights)\n",
    "fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc1_weights = np.array(fc1_weights)\n",
    "fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "fc2_weights = np.array(fc2_weights)\n",
    "\n",
    "num_layers = 5\n",
    "\n",
    "_, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "_, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "_, s_layer_2, _, = np.linalg.svd(conv3_weights)\n",
    "_, s_layer_3, _, = np.linalg.svd(fc1_weights)\n",
    "_, s_layer_4, _, = np.linalg.svd(fc2_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae72f18-e639-4408-a741-95151b72caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d6ead-4de5-4907-b640-037f51e08da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in zip(np.arange(num_layers), [s_layer_0, s_layer_1, s_layer_2, s_layer_3, s_layer_4]):\n",
    "    plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "    for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "        plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "plt.xticks(np.arange(num_layers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd88e6-ccbc-463e-8502-8364fd57a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_data = linkage(conv1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer1')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e212a-0411-4bd5-a0c8-a8811f3b09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer2')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb776-55aa-47e1-a710-1b3801e338e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(conv3_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer3')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d785aca-b39b-45d9-a812-406235b4ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc1_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: PLR')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3a20f-0f1a-45d8-8942-aa1916f9f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linkage_data = linkage(fc2_weights, method='single', metric='correlation')\n",
    "dendrogram(linkage_data)['dcoord']\n",
    "plt.title('Targeted-NIID: layer5')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c78f9d-cc6c-459d-8758-2404a5d83454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e286-f726-4044-95fb-ec25b21ab4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead0e8a-557b-4b3e-8d97-af9370d31ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8437cfc-68ed-48fc-8d82-9fc9bdfb4288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f315-00f2-4d04-87de-8acc99e07788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66663c2d-4265-4388-ac5c-715ee748a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_local_weights = [construct_ordered_dict(global_model, flat_weights) for flat_weights in malicious_grads]\n",
    "\n",
    "# conv1_weights = [np.array(w['conv1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv1_weights = np.array(conv1_weights)\n",
    "# conv2_weights = [np.array(w['conv2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# conv2_weights = np.array(conv2_weights)\n",
    "# fc1_weights = [np.array(w['fc1.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc1_weights = np.array(fc1_weights)\n",
    "# fc2_weights = [np.array(w['fc2.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc2_weights = np.array(fc2_weights)\n",
    "# fc3_weights = [np.array(w['fc3.weight'].detach().cpu().reshape(-1)) for w in structured_local_weights]\n",
    "# fc3_weights = np.array(fc3_weights)\n",
    "\n",
    "# num_layers = 5\n",
    "\n",
    "# _, s_layer_0, _, = np.linalg.svd(conv1_weights)\n",
    "# _, s_layer_1, _, = np.linalg.svd(conv2_weights)\n",
    "# _, s_layer_2, _, = np.linalg.svd(fc1_weights)\n",
    "# _, s_layer_3, _, = np.linalg.svd(fc2_weights)\n",
    "# _, s_layer_4, _, = np.linalg.svd(fc3_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3e820-c2be-48cc-b2da-f05321c4162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for x, y in zip(np.arange(num_layers-1), [s_layer_0, s_layer_1, s_layer_3, s_layer_4]):\n",
    "#     plt.scatter([x]*len(s_layer_0), y, cmap=\"copper\")\n",
    "#     for i, txt in enumerate(np.arange(len(s_layer_0))):\n",
    "#         plt.annotate(txt, (x, y[i]))\n",
    "    \n",
    "# plt.xticks(np.arange(num_layers))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506c011-6105-4c3e-8c2c-d49c25ba4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(args.mal_clients) > 0:\n",
    "#     import csv\n",
    "#     if args.iid:\n",
    "#         filename = 'Confidence_2IID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "#     else:\n",
    "#         filename = 'Confidence_2NIID_'+args.dataset+'_'+args.aggregation+'.csv'\n",
    "\n",
    "#     with open(filename, mode='w', newline='') as csvfile:\n",
    "#         w = csv.writer(csvfile)\n",
    "#         w.writerows(map(lambda x: [x], confidence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdac6fe-33d4-483e-97a8-3c6492435ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_mal_dataset_of_class\n",
    "\n",
    "test_mal_X_list, test_mal_Y, test_Y_true = get_mal_dataset_of_class(test_dataset, args.num_mal*100, Y_true, mal_Y)\n",
    "flat_test_mal_Y = np.hstack([y for y in test_mal_Y])\n",
    "mal_acc, mal_loss, mal_out = mal_inference(args, global_model, test_dataset, test_mal_X_list, flat_test_mal_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd663b-a50f-4009-b8bb-6d649683eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mal_acc, mal_loss, torch.mean(mal_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26323f6a-6e52-4e2e-901c-ffeb1c2d2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset, args.boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399b8e9-1931-4681-a07a-8e9e5a692ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_details(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4c375-638b-44ec-99b1-ee0de7e75a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### # PLOTTING (optional)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.figure()\n",
    "plt.title('Training Loss vs Communication rounds')\n",
    "plt.plot(range(len(train_loss)), train_loss, color='r')\n",
    "plt.ylabel('Training loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_loss.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))\n",
    "\n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Average Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(train_accuracy)), train_accuracy, color='k')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('../save/fed_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_acc.png'.\n",
    "            format(args.dataset, args.model, args.epochs, args.frac,\n",
    "                   args.iid, args.local_ep, args.local_bs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
